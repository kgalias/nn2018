{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big>Metody regularyzacji</big></big></big></big></big>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<id=tocheading><big><big><big><big>Spis treści</big></big></big></big>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"../nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization\n",
    "1. uczenie w mini-batchach\n",
    "  * mini-batche polepszają ewaluację gradientu\n",
    "  * uczenie szybsze niż przy pojedynczych przykładach\n",
    "2. uczenie sgd wymaga dopasowania wielu parametrów\n",
    "3. __covariate shift__\n",
    "  * sieć ma wiele warstw\n",
    "  * rozkład danych dla warstwy wejściowej jest stały (dla danego zbioru)\n",
    "  * w trakcie uczenia rozkład dla warstw kolejnych zmienia się\n",
    "  * ta zmiana tym bardziej im głębsza jest sieć\n",
    "  * wiele kolejnych warstw to jak złozenie funkcji, przy czym wewnątrzna __zmienia się__ w trakcie uczenia \n",
    "4. niech warstwa $Z$ ma wejście z warstwy $Y$: $$z=g(w^Ty+b)$$ a $g$ sigmoidalna\n",
    "  * wzrost wartości bezwzględnej $|w^T+b|$ prowadzi $g()$ do obszaru o niskim gradiencie\n",
    "  * spowoduje to ruch wielu wymiarów w kierunku saturacji\n",
    "5. użycie ReLU, dobrej inicjalizacji, małych prędkości może pomóc\n",
    "6. BN\n",
    "  * może pozwolic na uzycie nieliniowości sigmoidalnych\n",
    "  * powinno przyspieszyć uczenie\n",
    "    * pojedyncze epoki są wolniejsze, ale zbieżność szybsza\n",
    "    * możliwe większe prędkosci uczenia\n",
    "    * większa odporność na niedobrą inicjalizację\n",
    "    * zmniejszy wpływ zanikającego gradientu\n",
    "    * pozwala na użycie większej liczby rodzajów funkcji aktywacji\n",
    "    * ułatwia projektowanie sieci\n",
    "    * jest rodzajem generalizacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Założenia\n",
    "1. dawno wskazywano, że uczenie jest szybsze gdy wejścia są ___whitened___ (LeCun)\n",
    "  * mają zerowe średnie\n",
    "  * jednostkowe wariancje\n",
    "2. dość proste do realizacji dla danych wejściowych (chociaż pracochłonne)\n",
    "  * niech __wszystkie__ wejścia mają dodatnie wartości\n",
    "  * wszystkie wagi neuronów w pierwszej warstwie będą się __wspólnie__ zwiększać lub zmniejszać\n",
    "  * ścieżka do minimum będzie powolna \n",
    "  * kroki przetwarzania wejść\n",
    "    * usunięcie średnich\n",
    "    * dekorelacja (np. PCA)\n",
    "    * wyrównanie kowariancji by dla różnych wejść były w przybliżeniu równe\n",
    "    * byłoby fajnie dla kazdej warstwy\n",
    "3. inicjalizacja\n",
    "  * wrócimy do tego!!!!!!!!!!\n",
    "1. pełna normalizacja jest trudna __dla wszystkich cech__\n",
    "  * niech $x$ będzie wyjściem z danej warstwy, wtedy normalizacja\n",
    "  $$\\widehat{x}=Norm(x,X)$$\n",
    "  gdzie $X$ jest całym zbiorem\n",
    "  * to wymaga policzenia dla wstecznej propagacji\n",
    "  $$\\frac{\\partial\\,Norm(x,X)}{\\partial\\,x}\\hskip{2em}\\text{oraz}\\hskip{2em}\\frac{\\partial\\,Norm(x,X)}{\\partial\\,x}$$\n",
    "  * potem macierzy kowariancji $Cov[x]$ oraz $Cov[x]^{-1/2}$\n",
    "  * to jest co najmniej pracochłonne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "1. ___uproszczenie___ normalizacja dla każdej $k$-tej cechy osobno\n",
    "$$\\widehat{x}_k=\\frac{x_k-E[x_k]}{\\sqrt{Var[x_k]}}$$\n",
    "  * przyspiesza nawet jeśli cechy są od siebie zależne\n",
    "2. BN zmienia postać funkcji uczonej przez warstwę przyjmującą $x$\n",
    "  * aby __warstwa BN__ potrafiła wykonywać identyczność uczymy\n",
    "  $$y_k=\\gamma_k\\widehat{x}_k+\\beta_k$$\n",
    "3. ___uproszczenie___ każdy __mini-batch__ estymuje średnią i wariancję każdej aktywacji\n",
    "  * efektywniejsze niż dla całego zbioru\n",
    "    * psuje efekt SGD\n",
    "  * mini-batch często (zwykle) jest mniejszy niż liczba cech\n",
    "    * obliczanie pełnej kowariancji wymagałoby regularyzacji\n",
    "4. Obliczenia dla mini-batchu $B$ o $m$ przykładach (dla pojedynczej cechy $i$)\n",
    "$$\\begin{align}\n",
    "\\mu_B&=\\frac{1}{m}\\sum_i^mx_i\\\\\n",
    "\\sigma_B^2&=\\frac{1}{m}\\sum_i^m(x_i-\\mu_B)\\\\\n",
    "\\widehat{x}_i&=\\frac{x_i-\\mu_B}{\\sqrt{\\sigma_B^2+\\epsilon}}\\\\\n",
    "y_i&=\\gamma\\widehat{x}_i+\\beta\n",
    "\\end{align}$$\n",
    "  * końcową transformacją BN() jest $$y_k=BN_{\\gamma, \\beta}(x_k)=\\gamma_k\\widehat{x}_k+\\beta_k$$\n",
    "  gdzie $y_k$ są wyjściami z warstwy BN\n",
    "  * wartości $\\widehat{x}_i$ są __wewnątrz__ warstwy BN, ale to ich cechy są istotne\n",
    "    * $\\widehat{x}_i$ mają średnią $0$ i wariancję $1$\n",
    "    * wyjście $y$ staje się wejściem do liniowej sieci $\\gamma\\widehat{x}_i+\\beta$\n",
    "    * potem następują kolejne warstwy oryginalnej sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization a SGD\n",
    "1. użycie BN wymaga uwzglednienia w algorytmie uczenia\n",
    "  * transformacje są proste, gradienty proste do wyliczenia\n",
    "  * możliwe do uczenia\n",
    "  * transformacja liniowa - sieć zachowuje swoją pojemność\n",
    "2. przekształcenia BN __nie są__ potrzebne w trakcie inferencji\n",
    "  * po fazie uczenia stosujemy w warstwie BN normalizację __na całym zbiorze uczącym__\n",
    "  $$\\widehat{x}=\\frac{x-E[x]}{\\sqrt{Var[x]}}$$\n",
    "3. cały algorytm\n",
    "  1. uczenie warstwy BN\n",
    "  2. dodanie transformacji $y_k=BN_{\\gamma_k,\\beta_k}(x_k)$\n",
    "  3. użycie $y_k$ zamiast $x_k$\n",
    "  4. uczenie calej sieci\n",
    "  5. w sieci inferencji chwilowo obliczone parametry $\\gamma,\\beta$\n",
    "  6. po zakończeniu uczenia uśrednianie (moving window) parametrów po wielu mini-batchach\n",
    "  $$E[x]=E_B[\\mu_B]\\hskip{2em}Var[x]=\\frac{m}{m-1}E_B[\\sigma_B^2]$$\n",
    "  7. w warstwie BN zastąpienie $y=BN_{\\gamma,\\beta}$ przez\n",
    "  $$y=\\frac{\\gamma}{\\sqrt{Var[x]+\\epsilon}}+\\left(\\beta-\\frac{\\gamma\\,E[x]}{Var[x]+\\epsilon}\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gdzie umieścić BN?\n",
    "1. bezpośrednio przed obliczeniem nieliniowej aktywacji\n",
    "  * normalizujemy $x=w^Tu+b$, by z tego policzyć aktywację $g()$\n",
    "  * właściwie _bias_ może być ominięty, ponieważ normalizacja go zniweluje odejmując średnią\n",
    "2. BN można też wykorzystać w sieciach konwolucyjnych\n",
    "  * trzeba zadbać, by te same cechy w różnych miejscach były normalizowane w ten sam sposób"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zalety\n",
    "1. BN pozwala na wyższe współczynniki uczenia\n",
    "  * zwykle wysokie współczynniki uczenia prowadzą do wybuchu lub też zanikania gradientów\n",
    "  * BN zabezpiecza sieć przed utknięciem w rejonie saturacji\n",
    "2. BN regularyzuje model\n",
    "  * każdy przykład jest uczony w kontekscie swojego mini-batchu\n",
    "  * sieć nie zwraca deterministycznych wartości dla przykładów\n",
    "  * Dropout może być usunięty albo osłabiony\n",
    "    * różne są zdania\n",
    "  * można zredukować regularyzację L2\n",
    "3. przyspiesza uczenie\n",
    "  * zbieżność jest szybsza, co nadrabia z przewagą dodatkowe obliczenia\n",
    "4. potrzebne lepsze mieszanie przykładów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout \n",
    "1. pojedyncze neurony uczą się zbyt dokładnie (mogą się uczyć)\n",
    "  * overfitting\n",
    "2. sieć tworzy czasem zbyt dużą współzależność grup neuronów\n",
    "  * pary neuronów specjalizują się w rozpoznawaniu bardzo specyficznych problemów\n",
    "  * dropout __losowo__ wyłącza neurony\n",
    "  * neurony mają małą szansę na powtórzenie konfiguracji\n",
    "3. pozwala na uczenie badziej zgrubnych cech\n",
    "  * po wyłączeniu jedych, inne neurony muszą zastąpić tamte w rozpoznawaniu\n",
    "  * można zastosować dla neuronów wejściowych\n",
    "4. końcowy model jest __ensemblem__, przy czym liczba tworzonych modeli jest olbrzymia\n",
    "  * przy wyłączaniu tworzy się wiele niezaleźnych wewnętrznych reprezentacji\n",
    "5. zwykle wymaga więcej epok uczenia\n",
    "6. niekoniecznie dobrze współpracuje z BN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout - założenia\n",
    "1. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov\n",
    "2. najlepszym sposobem, przy nieograniczonych zasobach, będzie\n",
    "  * uśrednić predykcje po wszystkich ustawieniach parametrów\n",
    "  * ważyć każde przez posterior prawdopodobieństwa przy zadanych danych\n",
    "  * jest to zwykle możliwe tylko dla bardzo małych modeli\n",
    "3. przy łączeniu modeli w ensemble ważne by modele były __różne__ (ang. diverse)\n",
    "  * modele są różne, jeśli _popełniają błędy w różnych miejscach_\n",
    "  * uczenie sieci jest kosztowne\n",
    "4. Dropout stara się rozwiazać oba problemy\n",
    "  * czasowo odrzucane są poszczególne neurony\n",
    "    * widzialne lub ukryte\n",
    "  * prawdopodobieństwo dla ukrytych jest zwykle około $0.5$\n",
    "  * dla wejściowych nawet bliżej $1$!\n",
    "  * sieć staje się bardzo rozrzedzona\n",
    "5. pełna sieć to zbiór $2^n$ różnych zubożonych modeli\n",
    "  * dzielą jednak parametry\n",
    "  * połączenie słabych modeli może dawać model silny\n",
    "6. w jaki sposób ewoluować model?\n",
    "  * nie da się obliczyć wszystkich\n",
    "  * można wziąć jeden cały model\n",
    "    * wagi tego modelu są skalowanymi wersjami modelu pełnego\n",
    "    * można przemnożyć wagi przez prawdopodobieństwo, że waga była uzytwa w trakcie uczenia\n",
    "7. Dropout jest metodą regularyzacji przez dodanie szumu\n",
    "  * autoencoder dostaje zaszumione dane\n",
    "  * zadaniem jest je odtworzyć"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "1. sieć warstwowa\n",
    "$$\\begin{align}\n",
    "\\textbf{bez dropout}\\hskip{2em}&\\hskip{2em}\\textbf{z dropout}\\\\\n",
    "\\hskip{2em}&\\hskip{2em}r\\sim{}Bernoulli(p)\\\\\n",
    "\\hskip{2em}&\\hskip{2em}\\widetilde{y}^{(l)}=r*y^{(l)}\\\\\n",
    "z_i^{(l)}=w_i^{(l+1)}\\,y^{(l)}+b_i^{(l+1)}\\hskip{2em}&\\hskip{2em}z_i^{(l)}=w_i^{(l+1)}\\,\\widetilde{y}^{(l)}+b_i^{(l+1)}\\\\\n",
    "y_i^{(l+1)}=g(z_i^{(l+1)})\\hskip{2em}&\\hskip{2em}y_i^{(l+1)}=g(z_i^{(l+1)})\n",
    "\\end{align}$$\n",
    "2. uczenie\n",
    "  * dowolny algorytm typu SGD\n",
    "  * po wylosowaniu neuronów wsteczna propagacja __wyłącznie__ dla pozostawionych\n",
    "  * ważną formą regularyzacji jest __ograniczenie__ normy wektora wag wchodzącego do dowolnego neurona do ustalonej wartości\n",
    "    * tzw. __max-norm__ przez ograniczenie wektora do kuli o promieniu $c$\n",
    "  * także wysokie wartości __decay__ przyspieszają zbieżność\n",
    "3. Odtwarzanie\n",
    "  * wybranie modelu i przemnożenie wag\n",
    "  * ensemble\n",
    "    * wybrać $k$ modeli\n",
    "      * dla każdego postąpić znaleźć predykcje\n",
    "      * wszystkie wyniki uśrednić\n",
    "      * dla MNIST ok. $50$ modeli równa się z pełnym mnożonym\n",
    "      * zwykle wystarczy wziąć ok. $10$ niezależnych predykcji\n",
    "    * bardziej kosztowne\n",
    "4. Cechy uczenia\n",
    "  * uczenie wymaga zwykle sporo większej liczby epok\n",
    "  * neurony ukryte mają rzadką aktywację\n",
    "  * parametr prawdopodobieństwa pozostawienie neuronu $p$ można wybrać\n",
    "    * niskie $p$ (mało neuronów) powoduje niedofitowanie,  (dla MNIST)\n",
    "    * zwykle $p\\geq0.4$ daje najlepsze wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
