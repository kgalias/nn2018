{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big>Self Organizing Maps SOM</big></big></big></big></big>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<id=tocheading><big><big><big><big>Spis treści</big></big></big></big>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reguła Hebba\n",
    "> Jeśli akson neuronu A jest wystarczająco blisko neuronu B i regularnie bierze udział w jego aktywacji, wtedy następuje pewien proces wzrostu lub zmian metabolicznych w obu komórkach tak, że zdolność A do aktywacji B wzrasta (Donald Hebb, 1949)\n",
    "\n",
    "1. prosta interpretacja (McKay)\n",
    "  * niech sygnał $m$ (zapach banana) powoduje wzrost aktywacji neuronu $m$\n",
    "  * niech sygnał $n$ (żółty kształt) będzie związany z aktywacją neuronu $n$\n",
    "  * jeśli te dwa sygnały będą występowały jednocześnie, to reguła __uczenia hebbowskiego__ spowoduje wzrost wag $w_{mn}$ i $w_{nm}$\n",
    "  * jeśli później pojawi się jeden z tych sygnałów, to wywoła aktywację drugiego neuronu\n",
    "2. to jest uczenie __asocjacyjne__\n",
    "  $$\\Delta w_{ij}=\\eta x_i() y_j()$$\n",
    "  * jednoczesna aktywacja powoduje wzrost\n",
    "  * aktywacje asynchroniczne będą powodowały spadek\n",
    "  * wciąż powtarzające się aktywacje będą powodowały saturację\n",
    "    * waga przestaje się uczyć\n",
    "    * potrzebne ograniczenia\n",
    "3. to proces __samo-wzmocnienia__\n",
    "  * uczenie __nienadzorowane__\n",
    "4. __ograniczona__ liczba zasobów prowadzi do __współzawodnictwa__\n",
    "  * pomiędzy neuronami bądź grupami neuronów\n",
    "  * wybierane są te szybciej rosnące\n",
    "  * neurony nie aktywowane zanikają\n",
    "    * wraz z nie używanymi wagami\n",
    "  * powstaje __zwycięzca__ w procesie __winner-takes-all__\n",
    "    * zwycięzca staje się __detektorem cech__\n",
    "5. neurony będące w bliskości do siebie __kooperują__\n",
    "  * aktywowany neuron wpływa na aktywację neuronów ze sobą powiązanych\n",
    "  * tworzą się obszary aktywacji\n",
    "  * obszary aktywacji są grupami detektorów\n",
    "  * kooperacja jest związana ze współzawodnictwem\n",
    "6. __samo-organizacja__\n",
    "  * kooperacja tworzy grupy aktywacji\n",
    "  * rozdzielne grupy aktywacji tworzą strukturę systemu\n",
    "7. w mózgu ssaków tworzą się obszary _Brodmana_ \n",
    "  * charakteryzujące się różną budową histopatologiczną\n",
    "  * odpowiedzialne za różne funkcje\n",
    "\n",
    "<img src=\"nn_figures/juliabuntainecerebralmap.jpg\" width=\"75%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapy Kohonena\n",
    "<img src=\"nn_figures/som.png\" width=\"55%\" >\n",
    "1. organizacja\n",
    "  * neurony zwykle na 2-wymiarowej kwadratowej lub heksagonalnej siatce\n",
    "  * każdy neuron ma __sąsiadów__ w odległości $1$ na siatce\n",
    "2. każdy neuron ma związany wektor $w\\in\\mathbb{R}^n$, gdzie $n$ jest wymiarem wejścia\n",
    "  * dla wejścia $x$, każdy neuron oblicza $w_k^Tx$\n",
    "  * wszystkie $w_k$ mają taką samą długość\n",
    "3. __współzawodnictwo__ zwycięzcą jest neuron o największej wartości iloczynu skalarnego\n",
    "  * maksymalizacja $w^Tx$ jest równoważna __minimalizacji odległości__ $d(x,w_k)$\n",
    "  $$i(x)=\\underset{j}{\\arg\\,\\min}\\|x-w_j\\|$$\n",
    "  * wybór zwycięzcy odpowiada rzutowaniu ciągłej przestrzeni wejść na dyskretną mapę SOM\n",
    "4. __kooperacja__ zwycięzca i jego sąsiedzi będą współpracowały: ich wagi będą modyfikowane w tym samym kierunku\n",
    "<img src=\"nn_figures/som-learn.png\" width=\"75%\" >\n",
    "  * __otoczenie__ $h_{ji(x)}$ dla zwycięzcy $i()$ jest wycentrowane na zwycięzcy i malejące \n",
    "    * gausowskie, trójkątne, etc.\n",
    "    * mapa heksagonalna lepiej interpretuje odległość\n",
    "    * w otoczeniu _odległość_ jest liczona __na siatce__, __nie w przestrzeni wejściowej__\n",
    "    $$\\begin{align}\n",
    "    h_{ji(x)}(t)&=\\exp\\left(-\\frac{d_{ji(x)}^2}{2\\sigma^2(t)}\\right)\\\\\n",
    "    \\sigma(t)&=\\sigma(0)\\exp\\left(-\\frac{t}{\\tau}\\right)\n",
    "    \\end{align}$$\n",
    "      gdzie $t$ to krok, $\\tau$ stała początkowa\n",
    "      * zwykle sąsiedztwo obejmuje na początku całą mapę\n",
    "5. __adaptacja__ zwycięzca i jego sąsiedzi będą w kolejnych krokach aktywaniej reagowały na podobny sygnał\n",
    "  * uczenie jest hebbowskie\n",
    "    * prowadzi do saturacji, więc potrzebne są ograniczenia - czynnik __zapominania__\n",
    "    $$\\Delta w_j=\\eta y_j x - g(y_j) w_j$$\n",
    "    * pierwszy składnik jest uczeniem hebbowskim\n",
    "    * niech $y_j=h_{j i(x)}$\n",
    "      * dla zwycięzcy $y_{i(x)}=0$\n",
    "    * stąd pełne nauczanie\n",
    "    $$\\begin{align}\n",
    "    \\Delta w_j&=\\eta(t) y_j x - g(y_j) w_j\\\\\n",
    "    &=\\eta(t) y_j (x - w_j)\n",
    "    \\end{align}$$\n",
    "    dla $\\eta(t)=\\eta(0)\\exp\\left(-\\frac{t}{\\nu}\\right)$\n",
    "    * jest cały zestaw hipeparametrów: $\\eta(t), \\sigma(t)$\n",
    "6. __samo-organizacja__ zwycięzca wraz z sąsiadami będą tworzyć grupę\n",
    "7. w trakcie uczenia SOM przechodzi kilka etapów\n",
    "  * wagi są początkowo losowane\n",
    "  * początkowo nie ma więc żadnej organizacji\n",
    "  * pierwszym etapem jest faza __wyprostowywania__\n",
    "    * zbyt duża prędkość może uniemożliwić nawet wyprostowanie mapy\n",
    "    * staje się on niemożliwy, gdy sąsiedztwo jest zbyt małe\n",
    "  * w tym momencie sieć obejmuje tylko mały fragment wejścia\n",
    "  * teraz jest etap __rozciągania__ do całkowitej reprezentacji\n",
    "  * w końcu etap szczegółowego __dopasowywania__ do rozkładu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Koder i dekoder\n",
    "1. __Koder__ mapuje przestrzeń wejściową na dyskretną mapę SOM\n",
    "2. __dekoder__ odtwarza kody mapy SOM z powrotem\n",
    "  * wagi neuronu są zdeterminowane przez powtarzające się wejścia\n",
    "  * odpowiadają centrom klastrów\n",
    "  * tworzą dość dobrą aproksymację przestrzeni wejściowej\n",
    "3. mapa SOM jest __topologicznie uporządkowana__\n",
    "  * podobne wejścia są mapowane na ten sam lub nieodległe na siatce neurony\n",
    "  * neurony blisko na mapie mają podobne wagi\n",
    "  * grupy nieodległych neuronów można etykietować podobnymi cechami\n",
    "4. SOM powinna dobrze przybliżać rozkład prawdopodobieństwa przestrzeni wejściowej\n",
    "  * oszacowania mówią, że liczba neuronów odpowiadających małemu obszarowi jest proporcjonalna do $p_X^{1/x}$ do $p_X^{2/3}$\n",
    "    * małe obszary wejścia są nadreprezentowane\n",
    "    * duże są niedoreprezentowane\n",
    "    \n",
    "<img src=\"nn_figures/som-movies.png\" width=\"105%\" >\n",
    "[mapa SOM dla 10 tysięcy stron webowych związanych z rozrywką]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametry\n",
    "1. wybór wymiarów siatki jest trudny\n",
    "  * zbyt dokładna powoduje długi czas uczenia\n",
    "  * zbyt rzadka nie da dokładności\n",
    "2. możliwa jest adaptacja\n",
    "  * rozpoczynamy od stosunkowo małej siatki\n",
    "  * uczenie przechodzi fazę rozprostowywania\n",
    "  * w każdym węźle zwycięzcy przechowywane są statystyki __błędów__ $$\\sum\\|x-w_{i(x)}\\|$$\n",
    "  * gdy sumaryczny błąd wszystkich węzłów przekroczy ustalony próg\n",
    "    * wyszukiwane są wiersz/kolumny o największych sumarycznych błędach\n",
    "      * będą to te dla których błąd był duży oraz były często zwycięzcami\n",
    "    * __pomiędzy__ dwa wiersze/kolumny wstawiany jest __nowy__ wiersz/kolumna\n",
    "      * wagi inicjowane są na średnie\n",
    "      * błędy są dzielone przez $2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "Bardzo fajne demo autora GNG [Bernda Fritzke](https://www.demogng.de/js/demogng.html?model=SOM&showAutoRestart&autoRestartTime=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Growing Neural Gas\n",
    "1. siatka rozpoczyna od dwóch neuronów połączonych krawędzią\n",
    "  * krawędź ma parametr __wieku__\n",
    "2. zostaje wylosowany $x\\sim P_X$\n",
    "  * neurony zostają posortowane według rosnącej __odległości__ od $x$\n",
    "    * dla GNG __bez \"growing\"__ neuronów jest od razu N (niepołączonych) i są adaptowane wszystkie w coraz mniejszym stopniu\n",
    "  * wszystkie krawędzie są postarzone o $1$\n",
    "  * u zwycięzcy aktualizowany jest błąd $$err(s_1)=err(s_1)+\\|w_1-x\\|^2$$\n",
    "  * dla wszystkich węzłów połączonych z $s_1$ następuje aktualizacja\n",
    "    $$\\begin{align}\n",
    "    \\Delta w_1&=\\epsilon_1(x-w_1)\\\\\n",
    "    \\Delta w_j&=\\epsilon_j(x-w_j)\n",
    "    \\end{align}$$\n",
    "  * dla pierwszego $s_1$ i drugiego najbliższego węzła $s_2$\n",
    "    * jeśli __nie są__ połączone krawędzią, to jest ona tworzona\n",
    "    * wiek krawędzi $e(s_1, s_2)$ jest zerowany\n",
    "  * wszystkie krawędzie o wieku większym od maksymalnego są __usuwane__\n",
    "    * jeśli powoduje to utworzenie izolowanych węzłów, to są one także usuwane\n",
    "3. co ustaloną liczbę kroków\n",
    "  * wyszukiwany jest węzeł o największym zakumulowanym błędzie $s_f$\n",
    "  * wśród jego sąsiadów wyszukiwany jest ten o największym błędzie $s_s$\n",
    "  * pomiedzy te dwa wstawwiany jest nowy węzeł $$s_n=(s_f+s_s)/2$$\n",
    "    * wstawiane są krawędzie $e(s_f,s_n)$ i $e(s_n,s_s)$\n",
    "    * usuwana krawędź $e(s_f,s_s)$\n",
    "  * błąd $s_f$ i $s_s$ jest zmniejszany przez podzielenie przez stałą\n",
    "4. uczenie trwa aż do osiągnięcia maksymalnej liczby węzłów oraz stabilizacji błędu\n",
    "5. rzeczywiście dynamiczny algorytm\n",
    "  * działa w dynamicznym środowisku\n",
    "  * rzeczywiście uczy się topologii\n",
    "    * lepsze niż SOM Kohonena przybliżanie rozkładów\n",
    "  * warunki stopu mogą być czasem trudne do łatwego ustalenia\n",
    "  * może być czasochłonna\n",
    "6. problemem zarówno SOM jak i GNG jest trudność z dopasowywaniem rozkładów na brzegu siatki\n",
    "  * można spróbować torus\n",
    "6. Bardzo dobra [implementacja](https://github.com/kudkudak/Growing-Neural-Gas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
