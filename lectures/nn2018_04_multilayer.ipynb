{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big>Głębokie sieci warstwowe</big></big></big></big></big>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<id=tocheading><big><big><big><big>Spis treści</big></big></big></big>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "#from bokeh.io import gridplot, output_file, show\n",
    "#from bokeh.plotting import figure, output_notebook\n",
    "#from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"../nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Głębokie sieci warstwowe\n",
    "1. w jaki sposób nauczyć pojedyncze neurony?\n",
    "  * bardzo podstawowe modele nieskończenie wymiarowe\n",
    "    * wykorzystywane przez sieci RBF\n",
    "    * jesli $\\phi(x)$ będzie wystarczająco wysoko wymiarowe, to dopasuje się do zbioru uczącego\n",
    "    * generalizacja słaba\n",
    "  * wyliczyć je ręcznie\n",
    "  * wykorzystać modele __głębokie__ i nauczyć się $\\phi()$\n",
    "    * typowo używamy funckji aktywacji _rectified linear_ __ReLU__$(x)=max(0, x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje kosztu\n",
    "1. __maximum likelihood__\n",
    "$$L(w)=\\mathbb{E}_{x,y\\sim\\widehat{p}_{DATA}}\\log\\,p_{model}(y\\mid x)$$\n",
    "  * jeśli zakładmy konkretny rozkład prawdopodobieństwa warunkowego, to dostaniemy konkretną funkcję kosztu\n",
    "  * dla $p_{model}(y\\mid x)=\\mathcal{N}(y; f(x;w), \\mathbf{I})$ dostaniemy \n",
    "  $$L(w)=\\frac{1}{2}\\mathbb{E}_{x,y\\sim\\widehat{p}_{DATA}}\\|y-f(x;w\\|^2+const$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurony wyjściowe\n",
    "### __liniowe__ $y=w^Th+b$\n",
    "  * dla średniej warunkowego rozkładu gausowskiego $$p(y\\mid x)\\mathcal{N}(y; f(x;w), \\mathbf{I})$$\n",
    "    * można też przewidywać macierz kowariancji $\\Sigma$\n",
    "      * jednak musi być dodatnio określona\n",
    "      * zwykle ogranicza się do $\\Sigma=\\sigma\\mathbb{I}$\n",
    "      \n",
    "### __sigmoidalne__ dla rozkładu Bernoulliego $y=\\sigma(W^Th+b)$\n",
    "  * nigdy nie osiąga wartosci granicznych\n",
    "  * użycie liniowego obiętego do przedziału $[0,1]$ jest trudne w uczeniu\n",
    "    * po przejściu przez wartości graniczne gradient byłby zerowy\n",
    "\n",
    "### __softmax__ dla wielowymiarowego rozkładu Bernoulliego\n",
    "  * dla zmiennych dyskretnych o $n$ możliwych wartościach\n",
    "    * warstwa liniowa $z=w^Th+b$\n",
    "    gdzie $z_i=\\log\\,P(y=i\\mid x)$\n",
    "    * aby znaleźć $\\widehat{y}$ $$softmax\\,(z)_i=\\frac{\\exp(z_i)}{\\sum_j\\exp(z_j)}$$\n",
    "    * będzie działac prawidłowo przy uczeniu przez maximum log-likelihood\n",
    "    $$\\log\\,softmax(z)_i=z_i-\\log\\,\\sum_j\\exp(z_j)$$\n",
    "      * pierwszy składnik będzie pchany do góry, pozostałe w dół\n",
    "    * drugi składnik można aproksymować \n",
    "    $$\\log\\,\\sum_j\\exp(z_j)\\simeq\\max_j\\,z_j$$\n",
    "      * najsilniej będzie karana największa z niepoprawnych odpowiedzi\n",
    "      * jeśli największą wartość ma już $z_i$, to\n",
    "      $$\\log\\,\\sum_j\\exp(z_j)\\simeq\\max_j\\,z_j=z_i$$\n",
    "      i oba składniki będą się praktycznie zrowały\n",
    "        * przykład nie bedzie miał wielkiego wpływu na uczenie\n",
    "  * może być użyty dla neuronów ukrytych\n",
    "    * by neuron wybierał jedną z wielu wartości\n",
    "  * funkcje celu różne od _log-likelihood_ __nie będą__ dobrze współpracowały z _softmax_\n",
    "    * w szczególności będzie nią błąd kwadratowy\n",
    "  * ustabilizowana wersja softmax\n",
    "    * dla dużych wartości $z$, softmax bedzie się wypłaszczał bardzo\n",
    "    * _softmax_ jest odporny na translację_\n",
    "    * można uniezależnić od ekstremalnych wartosci przez\n",
    "    $$softmax(z)=softmax(z-\\max_j\\,z_j)$$\n",
    "    \n",
    "### __gausowskie__\n",
    "* może reprezentować warunkowy rozkład gausowski dla $y$ pod warunkiem $x$\n",
    "  * dla ustalonego $\\sigma$\n",
    "  * dla uczonej macierzy kowariancji $\\Sigma$\n",
    "    * rzadko kiedy innej niż diagonalna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurony __ukryte__\n",
    "* nigdy nie ma całkowitej pewnosci neurony którego typu będą działały najlepiej\n",
    "* nie wszystkie typy są __wszędzie__ różniczkowalne\n",
    "  * __ReLU__ $\\max(0,x)$ nie jest w $0$\n",
    "  * nie oczekujemy, że sieć rzeczywiscie dotrze do punktu o gradiencie $0$\n",
    "    * stąd punkty o nieokreślonym gradiencie są dopuszczalne\n",
    "    * jeśli pochodne lewa i prawa są różne, to oprogramowanie zwykle zwraca jedną z nich\n",
    "    * rzeczywista wartość zwykle i tak różni się o $\\epsilon$\n",
    "  * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit __ReLU__\n",
    "1. aktywacja $$g(z)=\\max(0, z)$$\n",
    "  * pochodna dodatnia kiedykolwiek neuron jest aktywny\n",
    "  * druga pochodna prawie wszędzie jest zerowa\n",
    "2. neurony __nie uczą__ się w miejscach, gdzie nie są aktywne\n",
    "  * niezerowe nchylenie dla $z_i<0$\n",
    "  $$g(z;\\alpha)_i=max(0,z_i)+\\alpha_i\\min(0, z_i)$$\n",
    "  * do wykorzystania, gdy cechy są inwariantne na odwrócenie odczytu, np. całkowita zmiana oświetlenie\n",
    "    * __stałe__ $\\alpha_i$ w __Leaky ReLU__\n",
    "    * __parametryczne__ $\\alpha_i$ gdy $\\alpha_i$ jest douczane\n",
    "3. __maxout__ \n",
    "  * złożenie wielu wartości liniowych $z_i$\n",
    "  $$g(z)_i=\\max_{j\\in{}Ind(i)}z_j$$\n",
    "  * maxout uczy się odcinkowo liniowej wypukłej funkcji\n",
    "4. wybór przez założenie o prostocie\n",
    "  * łatwiej uczyć modele, jesli są bardziej podobne do liniowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurony ukryte z sigmoidalnymi funkcjami aktywacji\n",
    "1. __logistyczna $g(z)=\\sigma(z)$\n",
    "2. __tanh__ $$g(z)=\\tanh(z)=2\\sigma(z)-1$$\n",
    "  * saturuje podobnie jak logistyczna\n",
    "  * w okolicach zera uczenie przypomina uczenie sieci liniowych\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inne neurony ukryte\n",
    "1. jest bardzo wiele funkcji aktywacji\n",
    "2. __funkcja liniowa__\n",
    "  * zwykle tylko nietóre\n",
    "3. __softmax__\n",
    "  * rzadko, ale mogą wymusić pewne wybory\n",
    "4. __radialne__\n",
    "5. __softplus__\n",
    "$$g(z)=\\log(1+\\exp(z))$$\n",
    "  * wydaje się wygładzoną wersją ReLU\n",
    "  * doświadczalnie sprawuje się słabiej\n",
    "5. __ograniczont $\\tnh$__\n",
    "$$g(z)=\\max(-1,\\min(1,z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Głębokie sieci\n",
    "> __Ogólne twierdzenie o aproksymacji__ mówi, że sieć warstwowa, z liniowymi neuronami ukrytymi oraz jedną warstwą ukrytą ze _zgniatającą_ funkcją aktywacji (np. logistyczną) jest w stanie aproksymować z dowolną dokładnością dowolną mierzalną funkcję ciągłą na domknietym podzbiorze $\\mathbb{R}^n$ pod warunkiem, że sieć będzie miała __wystarczającą__ liczbe neuronów ukrytych.\n",
    "\n",
    "* liczba neuronów ukrytych jest zwykle wykładniczo duża\n",
    "* sieć będzie potrafiła __aproksymować__ funkcję, ale nie ma żadnej gwarancji, że się jej __nauczy__\n",
    "* ale istnieje rodzina funkcji, które mogą być przedstawione pod warunkiem odpowiedniej __głębokości__\n",
    "* empirycznie wiemy, że większa głębokość daje zwykle lepszą generalizację"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularyzacja\n",
    "Celem regularyzacji jest osiągniecie modelu o niższej złożoności.\n",
    "* zwykle celem jest minimalizacja __wariancji__ kosztem nieco zwiększonego __biasu__\n",
    "\n",
    "## Regularyzacja przez czynnik kary\n",
    "$$L(w; X, y)=L(w; X, y)+\\alpha\\Omega(w)$$\n",
    "* algorytm uczący bedzie minimalizował koszt na danych\n",
    "* oraz jakąś miarę na parametrach $w$\n",
    "  * __nie__ regularyzujemy biasów\n",
    "  * wagi kontrolują interakcję między dwoma neuronami\n",
    "  * biasy tylko jednego neuronu\n",
    "  * biasy wprowadzają mniej wariancji w modelu\n",
    "* może się okazać potrzebne __oddzielne__ regularyzowanie różnych warstw\n",
    "  * różne $\\alpha$ dla róznych warstw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L2$\n",
    "$$\\Omega(w)=\\frac{1}{2}\\|w\\|_2^2$$\n",
    "* ridge regression albo regresja Tikhonova\n",
    "$$\\begin{align}\n",
    "w&=w-\\epsilon(\\alpha{}w+\\nabla_wL(w))\\\\\n",
    "&=(1-\\epsilon\\alpha)w-\\epsilon\\nabla_wL(w)\n",
    "\\end{align}$$\n",
    "\n",
    "Regularyzacja L2 powoduje __skalowanie__ optymalnego wektora wag $w^*$ __wzdłuż__ wektorów głównych zdefiniowanych przez macierz Hesjanu $H$\n",
    "<img src=\"nn_figures/L2.pdf\" width=\"100%\"> [Goodfellow et al.]\n",
    "1. wektor $w*$ jest przeskalowany wzdłuż $i$-tego wektora głównego $H$ przez czynnik $\\dfrac{\\lambda_i}{\\lambda_i+\\alpha}$, gdzie $\\alpha$ jest czynnikiem  funkcji kary\n",
    "  * jeśli $\\lambda_i>>\\alpha$, to regularyzacja będzie niewielka\n",
    "  * jeśli $\\lambda_i<<\\alpha$, to współrzędna będzie zredukowana prawie do minimum\n",
    "2. na rysunku\n",
    "  * $\\tilde{\\ w}$ jest punktem ekwilibrium miedzy minimum kosztu, a regularyzacją\n",
    "  * w pierwszym głównym kierunku wzdłuż $w_1$ wartość własna $H$ jest niewielka\n",
    "    * funkcja kosztu nie zmienia się wiele w tym kierunku, a wobec tego regularyzator ma duży wpływ i skraca $w_1$ do zera\n",
    "  * w drugim kierunku funkcja celu jest bardzo czula na zmiany\n",
    "    * wartość własna jest wysoka\n",
    "    * regularyzator mało wpływa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L1$\n",
    "Składnik kosztu jako $$\\Omega(w)=\\|w\\|_1=\\sum_i\\mid w_i\\mid$$\n",
    "* tylko wagi, nie bias\n",
    "* gradient $$\\nabla_wL(w)=\\nabla_wL(w)+\\alpha{}sign(w)$$\n",
    "* zachowanie jest inne niż $L2$\n",
    "  * gradient nie skaluje się liniowo z każdym $w$, stąd brak czystych rozwiązań\n",
    "  * rozwinięcie funkcji kosztu wraz z poprawką $L1$ \n",
    "  $$L(w)=L(w+(w-w^*))=L(w)+\\sum_i\\left[\\frac{1}{2}H(w-w^*)+\\alpha\\mid{}w_i\\mid\\right]$$\n",
    "    * skladnik pierwszej pochodnej znika, bo $w^*$ jest optimum\n",
    "  * rozwiązanie analityczne ma postać\n",
    "  $$w_i=sign(w_i^*)\\,\\max\\left[\\mid{}w_i^*\\mid-\\frac{\\alpha}{H_{i,i}},\\,0\\right]$$\n",
    "  zakładając, że $H$ jest diagonalna\n",
    "    * jeśli $\\mid{}w_i^*\\mid\\leq\\frac{\\alpha}{H_{i,i}}$\n",
    "      * tu rozwiązaniem jest $w_i=0$\n",
    "      * regularyzacja przeważa\n",
    "    * jeśli $\\mid{}w_i^*\\mid>\\frac{\\alpha}{H_{i,i}}$\n",
    "      * $w_i$ jest przesuwane\n",
    "* $L1$ daje bardziej __rzadkie__ rozwiązanie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rozszerzanie zbioru uczącego\n",
    "1. na pewno lepiej jest mieć więcej danych niż mniej\n",
    "2. sztuczne dane\n",
    "  * klasyfikator powinien być inwariantny na transformacje - wystarczy je zastosować\n",
    "  * ale ostrożnie gdy mogą one zmienić znaczenie: 6 a 9\n",
    "3. szum losowy\n",
    "  * także dodawany do neuronów ukrytych\n",
    "  * sieci bywają mało odporne na szum\n",
    "  * wykorzystywane w tzw. __denoising autoencoders__\n",
    "  * także __dropout__ jest formą dodawania szumu (właściwie __mnożenia__ przez szum)\n",
    "4. ostrożność przy porównywaniu algorytmów\n",
    "  * jeden algorytm działa słabo przy danych czystych\n",
    "  * drugi dobrze przy danych rozszerzonych\n",
    "  * jaki można wyciągnąć wniosek?\n",
    "  * ale pamiętac by warunki porównania były sprawiedliwe (ten sam zbiór dla porównań)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dodawanie szumu do neuronów wyjsciowych\n",
    "* zbiory mają błędy w etykietach\n",
    "* maksymalizacja $\\log\\,p(y\\mid x)$ może zaszkodzić, jesli etykieta $y$ jest błędna\n",
    "* jak temu zaradzić?\n",
    "  * modelować szum etykietowań\n",
    "  * załóżmy, że etykiety są poprawne z prawdopodobieństwem $1-\\epsilon$\n",
    "  * najlepiej __dodać__ to założenie do funkcji kosztu\n",
    "  * __label smoothing__\n",
    "    * zastąpienie w modelu opartym o _softmax_ etykiet $0$ i $1$ przez $\\frac{\\epsilon}{k}$ i $1-\\frac{k-\\epsilon}{k}$\n",
    "    * uczenie nigdy nie dotrze do etykiet $0$ i $1$\n",
    "    * będzie się długo uczyć ciągle __powiekszając__ wagi\n",
    "    * konieczne dodanie jakiegoś _weight decay_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie semi-nadzorowane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie wielo-zadaniowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
