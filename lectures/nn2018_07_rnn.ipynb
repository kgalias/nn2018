{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big>Sieci rekurencyjne</big></big></big></big></big>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<id=tocheading><big><big><big><big>Spis treści</big></big></big></big>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci rekurencyjne\n",
    "<img src=\"nn_figures/rnn.jpeg\" width=\"90%\"> [Nature]\n",
    "\n",
    "1. __rozwinięcie sieci__ w $n$ składowych dla wygenerowania $n$-elementowego ciągu\n",
    "2. $x_t$ to wejscie w chwili $t$\n",
    "  * gdy generujemy więcej niż jedno słowo (znak) wprzód, to poprzednio wygenerowane staje się wejściem do następnego\n",
    "3. __pamięć__ $s_t$ \n",
    "  * obliczane na podstawie poprzednich $s_t=f(Ux_t+Ws_{t-1})$\n",
    "  * $f$ funkcją nieliniową\n",
    "4. __wyjście__ $o_t$ w chwili $t$\n",
    "  * zwykle jako $softmax(Vs_t)$\n",
    "  * zwraca wektor prawdopodobieństw stanów dyskretnych\n",
    "  * interesujący może być np. tylko ostatni stan określający znaczenie zdania (sentiment analysis)\n",
    "5. $s_t$ przechowuje __całą__ informację na temat poprzednich stanów obliczeń\n",
    "  * praktycznie nie jest wystarczająca\n",
    "6. __wszystkie__ kroki __dzielą__ te same parametry $U, V, W$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemy RNN\n",
    "1. od dawna znane różne podstawowe architektury RNN\n",
    "  * stanem pamięci jest stan ukryty i tam następuje rekurencja\n",
    "  * aktualny stan wyjsciowy staje się _dodatkowym_ stanem wejściowym (jak w automatach)\n",
    "2. podstawowymi problemami są\n",
    "  * pamięć jedynie ostatnich akcji, _zapominanie_ stanów poprzednich\n",
    "  * pamięć jedynie pojedynczych stanów globalnych dla całego modelu bez pamięci stanów ostatnich\n",
    "  * stąd potrzeba modelu wypełniającego tą dziurę - __long-short time memory__\n",
    "  * eksplodujące / zanikające gradienty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zastosowania\n",
    "1. __modelowanie i generowanie języka__\n",
    "  * predykcja __prawdopodobieństwa__, że zdanie jest poprawne\n",
    "  * samplując z tego dostajemy model __generatywny__\n",
    "  * model językowy z użyciem __n-gramów__\n",
    "  $$P(w_1,\\dots,w_m)=\\prod_{i=1}^mP(w_i\\mid w_1,\\dots,w_{i-1})\\approx\\prod_{i=1}^mP(w_i\\mid w_{i-(n-1)},\\dots,w_{i-1})$$\n",
    "  dla n-gramów $$P(w_i\\mid w_{i-(n-1)},\\dots,w_{i-1})=\\frac{\\#(w_{i-(n-1)},\\dots,w_{i-1}, w_i)}{\\#(w_{i-(n-1)},\\dots,w_{i-1})}$$\n",
    "2. __tłumaczenie języka__\n",
    "  * podobne do modelowania\n",
    "  * wymaga zwykle przeczytania kompletnego zdania w jednym języku __przed__ wygenerowaniem pierwszego słowa nowego zdania\n",
    "3. __rozpoznawanie języka__\n",
    "  * wejściem są odczytane __fonemy__\n",
    "  * wyjściem nowe fonemy lub transkrypcja na zdania (tłumaczenie)\n",
    "4. Modele RNN pozwalają przyjmować wejścia o __zmiennej długości__\n",
    "  * na przykład opis obrazu jako wiele losowych sampli z niego\n",
    "\n",
    "<img src=\"nn_figures/rnn-diagrams.jpeg\" width=\"80%\"> [Karpathy]\n",
    "* od lewej do prawej\n",
    "  * zwykłe przetwarzanie \n",
    "  * tłumaczenie pojedynczego obiektu na opis (na przykład obraz na wiele związanych znim słów)\n",
    "  * __analiza sentymentu__ przyjmuje całą sekwencję i ocenia ją na końcu\n",
    "  * tłumczaenie maszynowe\n",
    "  * równoległe wejście-wyjście"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-Propagation Through Time BPTT\n",
    "1. za każdym razem patrzymy kilka kroków wstecz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# klasa RNN (za http://wildml.com)\n",
    "class RNNNumpy():\n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), \n",
    "                                   (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), \n",
    "                                   (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), \n",
    "                                   (hidden_dim, hidden_dim))\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    " \n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    " \n",
    "    #RNNNumpy.predict = predict\n",
    "    #RNNNumpy.forward_propagation = forward_propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT\n",
    "<img src=\"nn_figures/rnn.jpeg\" width=\"70%\"> [Nature]\n",
    "1. w każdym kroku należy znaleźć wszystkie macierze parametrów $U, V, W$\n",
    "  * są wspólne dla wszystkich kroków\n",
    "  * zwykle mają dużo parametrów\n",
    "  * niech będzie $N$ różnych słów, a pamięć jest reprezentowana przez wektor o długosci $K$\n",
    "    * $x_t\\in\\mathbb{R}^{N}$\n",
    "    * $o_t\\in\\mathbb{R}^{N}$\n",
    "    * $s_t\\in\\mathbb{R}^{K}$\n",
    "    * $U\\in\\mathbb{R}^{K\\times{}N}$\n",
    "    * $V\\in\\mathbb{R}^{N\\times{}K}$\n",
    "    * $W\\in\\mathbb{R}^{K\\times{}K}$\n",
    "2. parametry są __dzielone__ we wszystkich przewidywanych krokach\n",
    "  * gradient w aktualnym kroku zależy \n",
    "    * od obliczeń w aktualnym kroku czasu\n",
    "    * od obliczeń w poprzednim kroku\n",
    "  * odpowiada to wykorzystaniu __reguły łańcuchowej__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# za [Britz]\n",
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # wykonanie propagacji wprzód (zwraca ostatnie wyjscie i stan pamięci)\n",
    "    #  forward_propagation() wykonuje kroki wprzód zapamiętując wszystkie wartosci pośrednie,\n",
    "    #  które będą później potrzebne\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # macierze potrzebne dla akumulacji gradientów\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # teraz cofając się wstecz w obliczeniach\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # wstęczne obliczenia dla ostatniego kroku\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # wsteczna propagacja w czasie po poprzedzających krokach, ale co najwyżej bptt_truncate kroków\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # aktualizacja\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT\n",
    "1. Algorytm jest w stanie nauczyć się prostych zależności\n",
    "  * kolejność słów: bi-gramy, tri-gramy\n",
    "  * częstość występowania słów\n",
    "  * prostej składni\n",
    "  * prostej interpunkcji\n",
    "3. Jednak\n",
    "  *\n",
    "  * podawane zdania są zbyt krótkie by nauczyć poprawnej gramatyki\n",
    "  * dłuższe zdania znacznie zwiększają złożoność uczenia\n",
    "  * __nie jest w stanie__ nauczyć się zależności między __odległymi__ słowami\n",
    "    * proste RNN są w stanie imitować __jedynie__ pamięć krótko-terminową\n",
    "  * BPTT cierpi w dużym stopniu na problem zanikającego / eksplodującego gradientu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPTT koszt i wsteczna propagacja\n",
    "<img src=\"nn_figures/rnn.jpeg\" width=\"70%\"> [Nature]\n",
    "\n",
    "<img src=\"nn_figures/rnn-bptt1.png\" width=\"70%\"> [Nature]\n",
    "\n",
    "1. koszt\n",
    "$$E(y, \\widehat{y})=\\sum_tE_t(y_t,\\widehat{y}_t)$$\n",
    "2. dla $z_3=Vs_3$ mamy\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E_3}{\\partial V} &= \\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial\\widehat{y}_3}{\\partial V}\\\\\n",
    "&=\\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial\\widehat{y}_3}{\\partial z_3} \\frac{\\partial z_3}{\\partial V}\\\\\n",
    "&=(\\widehat{y}_3-y_3)\\otimes s_3\n",
    "\\end{align}$$\n",
    "3. dla pochodnej po $W$ zaczyna się pojawiać rekurencja\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E_3}{\\partial W} &= \\frac{\\partial E_3}{\\partial s_3} \\frac{\\partial s_3}{\\partial W}\\\\\n",
    "&= \\frac{\\partial E_3}{\\partial \\widehat{y}_3}\\frac{\\partial \\widehat{y}_3}{\\partial s_3} \\frac{\\partial s_3}{\\partial W}\\\\\n",
    "&\\hskip3em\\text{jednak $s_3$ bezpośrednio zależy od $s_2$, które nie jest stałe!}\\\\\n",
    "s_3&=\\tanh(U x_t+W s_2)\\\\\n",
    "\\frac{\\partial E_3}{\\partial W} &=\\sum_{t=0}^3\\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial \\widehat{y}_3}{\\partial s_3} \\frac{\\partial s_3}{\\partial s_t}\\frac{\\partial s_t}{\\partial W}\\\\\n",
    "\\end{align}$$\n",
    "<img src=\"nn_figures/rnn-bptt-gradients.png\" width=\"70%\"> [Nature]\n",
    "4. w rzeczywistości BPTT niewiele się różni od zwykłej wstecznej propagacji\n",
    "  * w sieci warstwowej parametry między warstwami __nie są__ dzielone\n",
    "  * nie ma potrzeby ich sumowania\n",
    "  * w analogiczny sposób można zdefiniować regułę delta\n",
    "  $$\\delta^3_2=\\frac{\\partial E_3}{\\partial z_2}=\\frac{\\partial E_3}{\\partial s_3}\\frac{\\partial s_3}{\\partial s_2}\\frac{\\partial s_2}{\\partial s_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPTT i zanikający gradient\n",
    "1. podstawowym problemem w uczeniu jest zanikanie gradientu\n",
    "  * problem zauważył Hochreiter, który był autorem modelu LSTM\n",
    "$$\\frac{\\partial E_3}{\\partial W} =\\sum_{t=0}^3\\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial \\widehat{y}_3}{\\partial s_3} \\frac{\\partial s_3}{\\partial s_t}\\frac{\\partial s_t}{\\partial W}$$\n",
    "2. w rozwiązaniu występuje czynnik\n",
    "$$\\frac{\\partial s_3}{\\partial s_t}$$\n",
    "  * i tak chociażby $$\\frac{\\partial s_3}{\\partial s_1} = \\frac{\\partial s_3}{\\partial s_2}\\frac{\\partial s_2}{\\partial s_1}$$\n",
    "  * skąd mamy\n",
    "  $$\\frac{\\partial E_3}{\\partial W} =\\sum_{t=0}^3\\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial \\widehat{y}_3}{\\partial s_3} \\left(\\prod_{j=t+1}\\frac{\\partial s_j}{\\partial s_{j-1}}\\right)\\frac{\\partial s_t}{\\partial W}$$\n",
    "  * $s_t=\\tanh(Ux_t+Ws{t-1})$\n",
    "  * $\\tanh()$ ma obszar saturacji po lewej i prawej stronie, a jego gradient maleje __eksponencjalnie__ szybko\n",
    "  * jeśli aktywacje są daleko od własciwych, to gradient spada prawie do zera\n",
    "  * wymnażanie bardzo małych wartosci tylko eksponencjalnie szybko je jeszcze zmniejsza...\n",
    "3. eksplodujący gradient pojawia się równie często\n",
    "  * jest efektem kilku wysokich aktywacji\n",
    "  * może prowadzić do oscylacji, gdy nadchodzące sygnały są sprzeczne\n",
    "  * w miarę łatwo sobie z nim poradzić przez obcinanie gradientu z wysoką normą\n",
    "4. a jak z zanikającym gradientem?\n",
    "  * trudniej: sieć nie uczy się wale albo potrzebuje wykładniczo wiele czasu\n",
    "  * poprawna inicjalizacja\n",
    "  * ReLU zamiast funkcji sigmoidalnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemy\n",
    "1. __krótka__ a __długa__ pamięć\n",
    "  * RNN z algorytmem typu BPTT szybko ___zapomina___ informacje\n",
    "  * korzysta tylko z ostatniej\n",
    "  * model dla angielskiego na poziomie znaków szybko nauczy się, że po znaku `q` __zawsze__ występuje znak `u`\n",
    "  * jednak nie nauczy się informacji kontekstowej z poprzedniego zdania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Time Memory LSTM\n",
    "1. w 1991 Sepp Hochreiter obronił pracę dyplomową w Monachium w której \n",
    "  * przedstawił szczegółową analizę uczenia sieci rekurencyjnych\n",
    "  * odkrył zjawisko zanikajacego i eksplodujacego gradientu\n",
    "2. w 1997 zaproponował, wraz z Jurgenem Schmidhuberem, model LSTM \n",
    "  * Neural Computation:9(8):1735-1780 \n",
    "  * wcześniej w 1995 w _technical document_ w Monachium\n",
    "3. model LSTM stał się początkiem dla wielu innych modeli\n",
    "  * rozwinięcia LSTM, np. GRU (dodatkowe bramki, inny przepływ, etc.)\n",
    "  * sieci warstwowe typu Highway\n",
    "  * sieci ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (za Hochreiterem)\n",
    "1. jako podstawowy problem Hochreiter zauważył ___zanikający sygnał błędu___\n",
    "  * błąd zanika i sieć nie uczy się niczego w sensownym czasie\n",
    "  * dla sigmoidalnych funkcji aktywacji wagi musiałyby być większe od $\\approx4$ by sygnał miał wystarczającą wartość\n",
    "  * jednak większych przy inicjalizacji wagi nic nie pomogą, bo odpowiednia pochodna maleje jeszcze szybciej\n",
    "  * BPTT jest bardzo czuły na ostatnie zmiany/sygnały\n",
    "  * Hochreiter zauważył, że konieczne jest zapewnienienie stałego przepływu sygnału błędu\n",
    "    * wniosek: funkcja aktywacji __musi być liniowa__\n",
    "2. komórka LSTM\n",
    "\n",
    "<img src=\"nn_figures/LSTM-Hochreiter.pdf\" width=\"80%\"> [Neural Computation]\n",
    "\n",
    "<img src=\"nn_figures/gers_lstm.png\" width=\"80%\"> [Hochreiter]\n",
    "\n",
    "  * dodaje (multiplikatywną) __bramkę wejściową__ $in$, która ma zabezpieczyć zawartość pamięci od wpływu _nieistotnych_ wejść\n",
    "  * analogicznie __bramkę wyjściową__ $out$ mającą zabezpieczyć inne komórki przed wpływem (aktualnych) nieistotnych informacji w komórce\n",
    "  * __czemu bramki ?__\n",
    "    * bramka wejsciowa $in$ kontroluje przepływ sygnału błędu by zabezpieczyć przed konfliktami wag\n",
    "      * czasem komórka _powinna_ uzyć wejścia z innej komórki\n",
    "      * czasem nie\n",
    "      * bramka wejsciowa kontroluje to\n",
    "    * podobnie bramka $out$ kontroluje wagi wyjściowe\n",
    "    * bramki wejściowa/wyjściowa muszą się __nauczyć__, które sygnały wyłapać/zablokować"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM przykład (za Hochreiterem)\n",
    "<img src=\"nn_figures/LSTM-Hochreiter-flow.pdf\" width=\"80%\"> [Neural Computation]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM krok po kroku (za Christopher Olah)\n",
    "### bramka wejściowa\n",
    "\n",
    "<img src=\"nn_figures/LSTM3-focus-f.png\" width=\"80%\"> [Colah]\n",
    "  * sigmoidalna bramka wejsciowa decyduje __które__ informacje będą aktualizowane\n",
    "    * wartość poprzedniej pamieci jest __wymnażana__ przez $f_t$\n",
    "    * $f_t$ jest bramką _zapominającą_ nieistotne w tej chwili informacje\n",
    "\n",
    "### aktualizacja stanu\n",
    "<img src=\"nn_figures/LSTM3-focus-i.png\" width=\"80%\"> [Colah]\n",
    "  * komórka decyduje które wartości z wejścia należy _dodać_ do tej aktualizowanej\n",
    "    * wartości $i_t\\widetilde{C}_t$ są dodawane do poprzednio wyczyszczonej i odpowiednio przeskalowane\n",
    "    * to decyduje, które informacje z wejścia (i stanu pamięci) należy teraz użyć\n",
    "  * te wartości połączone decydują o wyjsciu stanu komórki\n",
    "\n",
    "\n",
    "<img src=\"nn_figures/LSTM3-focus-C.png\" width=\"80%\"> [Colah]\n",
    "\n",
    "### wyjście\n",
    "\n",
    "<img src=\"nn_figures/LSTM3-focus-o.png\" width=\"80%\"> [Colah]\n",
    "  * po pierwsze bramka __wyjściowa__ (sigmoidalna) decydująca, które elementy stanu należy przekazać na wyjście $o_t$\n",
    "  * stan komórki jest reskalowany do $(-1,+1)$ przez $\\tanh$\n",
    "  * przeskalowany stan komórki jest filtrowany przez bramkę wyjsciową\n",
    "  \n",
    "### warianty\n",
    "* bramki wykorzystują wgląd w stan komórki\n",
    "\n",
    "<img src=\"nn_figures/LSTM3-var-peepholes.png\" width=\"80%\"> [Colah]\n",
    "  * nie zawsze wykorzystywane przez wszystkie bramki\n",
    "\n",
    "* połączenie bramek zapominającej i wejsciowej\n",
    "\n",
    "<img src=\"nn_figures/LSTM3-var-tied.png\" width=\"80%\"> [Colah]\n",
    "  * zapomina jedynie te składniki, w które zostanie wstawiona nowa informacja\n",
    "\n",
    "* Gated Recurrent Unit (Cho, 2014)\n",
    "\n",
    "<img src=\"nn_figures/LSTM3-var-GRU.png\" width=\"80%\"> [Colah]\n",
    "  * spore uproszczenie, a przez to staje się popularna\n",
    "  * połączenie bramek zapiminajacej i wejsciowej\n",
    "  * łączy stan komórki $C_t$ wraz ze stanem ukrytym komórki $h_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jakie parametry?\n",
    "1. uczenie sieci LSTM nie jest trudne\n",
    "  * jest bardzo wiele wariantów\n",
    "  * architektura nie jest oczywista i ma wiele składników możliwe, że na wiarę\n",
    "  * które są najlepsze i jak je uczyć?\n",
    "2. R. Jozefowicz, Zaremba i Sutskever wykonali duży przegląd możliwych architektur wyciągając szereg wniosków\n",
    "  * istnieje wiele architektur podobnych do GRU lepszych na wielu zadaniach\n",
    "  * LSTM: jest generalnie najlepszy jeśli użyty był Dropout\n",
    "  * LSTM: dodawanie jedynki do bramki zapominającej zwykle minimalizuje różnicę do najlepszych architektur\n",
    "    * typowa inicjalizacja ustawia wszystkie wagi na małe wartości\n",
    "    * inicjalizacja biasu bramki zapominajacej na małą wartość sprzyja pojawieniu się zanikającego gradientu\n",
    "    * rozwiązaniem może być inicjalizacja go na wyższe wartości rzędu $1 - 2$\n",
    "  * istotność bramek\n",
    "    * bramka wejściowa __jest__ istotna\n",
    "    * bramka wyjściowa __nie jest__ istotna\n",
    "    * bramka zapominająca __jest bardzo__ istotna dla wszystkich problemów __poza__ modelowaniem języka\n",
    "3. autorzy wykorzystali prostą procedurę przeszukiwania\n",
    "  * utrzymywali listę 100 znalezionych najlepszych architektur poszukując dla nich najlepszych hiperparametrów\n",
    "  * w każdym etapie wykonuje jeden z kroków\n",
    "    * losuje jedną ze 100 architektur\n",
    "      * ewaluuje 20 losowych ustawień hiperparametrów dla każdego z 3 zadań\n",
    "      * ocenia przez\n",
    "      $$\\min\\frac{\\text{najlepsza dokładność dla architektury dla zadania}}{\\text{najlepszy wynik GRU dla zadania}}$$\n",
    "      przy czym GRU były wyliczone dla wszystkich dozwolonych architektur\n",
    "    * wybiera jedną ze 100 architektur\n",
    "      * mutuje parametry\n",
    "  * autorzy\n",
    "    * ewaluowali 10 tysięcy różnych architektur\n",
    "    * 1000 z nich przeszło początkowy test zapamiętywania\n",
    "      * 5 znaków w sekwencji, dla wszystkich 26 możliwości\n",
    "      * jest czytanych w sekwencji\n",
    "      * i ma być odtworzone w tej samej sekwencji\n",
    "    * te 1000 architektur było sprawdzone dla średnio 2200 konfiguracji\n",
    "    * razem wykonali testy dla ok. 230 tysiecy konfiguracji\n",
    "    * zadania\n",
    "      * obliczenie sumy (w postaci znaków) dla sekwencji składajacej się z dwóch sekwencji\n",
    "      * predykcja następnego znaku w kodzie XML\n",
    "      * modelowanie języka Penn Tree-Bank\n",
    "      * (dodatkowo) modelowanie muzyki polifonicznej\n",
    "      \n",
    "  * rezultaty\n",
    "    * GRU było lepsze od LSTM na wszystkich zadaniach poza modelowaniem języka\n",
    "    * jeśli wykorzystano dropout, to LSTM było zdecydowanie najlepsze dla problemów modelowania jezyka\n",
    "    * LSTM z wysokim biasem bramki zapominajęcej było lepsze od innych LSTM i GRU na prawie wszystkich zadaniach\n",
    "    * trzy mutacje znalezione okazały się konkurencyjne do innych modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wielkość ukrytej warstwy\n",
    "1. $x_t$ i $h_t$ są wektorami\n",
    "  * jeśli rozpoznajemy znaki, to $h_t$ będzie wektorem prawdopodobieństw znaków\n",
    "  * w przypadku słów, trzeba wykorzystać jakiś word-embedding, by zredukować wymiarowość\n",
    "    * word-embedding pozwoli reprezentować słowa jako wektory w $\\mathbb{R}^K$\n",
    "    * word-embedding może być uczony __w trakcie__ uczenia modelu\n",
    "2. modele LSTM zawierają pojęcie __hidden layer__\n",
    "  * warstwa ukryta jest zbiorem neuronów w bramkach zapominajacych, wejsciowych, wyjsciowych, etc.\n",
    "  * ich wielkość to właśnie warstwa ukryta\n",
    "  * większa warstwa ukryta zapewnia modelowi większą pojemność\n",
    "3. innym typowym parametrem jest żądanie by model zwracał całe sekwencje\n",
    "  * w Keras to parametr `return_sequences`\n",
    " \n",
    "  <img src=\"nn_figures/Keras-LSTM-return-sequences.png\" width=\"80%\"> [Chollet]\n",
    "  * to pozwala budować sieci __wielopoziomowe__\n",
    "  <img src=\"nn_figures/LSTM-two-layer.png\" width=\"80%\"> [Colah]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci Highway\n",
    "1. Dla wielu problemów __głębsze__ sieci dają __lepsze__ wyniki\n",
    "  * wciąż problemem jest __zanikający gradient__\n",
    "  * potrzebne nowe modele sieci radzące sobie z tym\n",
    "  * rozwiązaniem może być wykorzystanie __bramek zapominających__ wziętych z LSTM\n",
    "    * początkowe rozwiązania Srivastavy były znacznie bardziej oparte na LSTM\n",
    "    * końcowe jest dużym uproszczeniem\n",
    "2. różnymi podejsciami są\n",
    "  * lepsze algorytmy uczące\n",
    "  * lepsze inicjalizacje\n",
    "  * lepiej dopasowane funkcje aktywacji\n",
    "  * połączenia skrótowe\n",
    "    * zawsze obecne w badaniach połączenie między warstwami\n",
    "      * przykładem sieć dla XOR\n",
    "    * połączenie __do__ warstw wyjściowych dodające szum\n",
    "    * dodatkowe funkcje kosztu w trakcie uczenie (patrz Inception)\n",
    "2. aktywacja nowej warstwy\n",
    "  * jest częściowo kopią aktywacji poprzedniej (a więc wejścia)\n",
    "  * częściowo nowo wyliczoną wartością\n",
    "  * połączenie jako kombinacja liniowa\n",
    "  * parametry kombinacji są __uczone__\n",
    "  $$\\begin{align}\n",
    "  y_i &= T(x, w_T)\\,\\,\\sigma\\left(\\sum_{j}w_{ij}x_j+b\\right)+(1-T(x; w_T))x\\\\\n",
    "  T&=\\sigma\\left(\\sum_{j}w_{T_{ij}}x_j+b_T\\right)\n",
    "  \\end{align}$$\n",
    "  gdzie $\\sigma()$ jest funkcją logistyczną\n",
    "  * $T$ to bramka __transfomacji__\n",
    "  * $1-T$ to bramka __przeniesienia__ (ang. carry)\n",
    "3. można tak ustawić $b'$, że $T$ będzie zmierzało do zera\n",
    "  * wtedy aktywacja następnej warstwy jest bliska poprzedniej\n",
    "  $$y = \n",
    "  \\begin{cases} x &\\mbox{if } T() = 0 \\\\ \n",
    "  \\sigma\\left(\\sum_{j}w_{ij}x_j+b\\right) & \\mbox{if } T() = 1 \n",
    "  \\end{cases}\n",
    "  $$\n",
    "  * informacja może przechodzić przez wiele warstw __bez jej _rozcieńczenia___\n",
    "  * Srivastava i Schmidhuber nazwali takie ścieżki __information highways__\n",
    "4. definicja wymaga by warstwy miały __takie same wymiary__!\n",
    "  * jeśli zmiana wymiaru jest konieczna, wtedy można\n",
    "    * zamienić wejście $x$ na __rozszerzone__ przez odpowiednie\n",
    "      * dopełnienie zerami\n",
    "      * odpowiednie wycięcie fragmentu\n",
    "    * dodać __zwykłą__ warstwę i później kontynuować warstwy typu highway\n",
    "  * działa podobnie dla sieci konwolucyjnych\n",
    "  * super! ale nic za darmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uczenie sieci Highway\n",
    "\n",
    "<img src=\"nn_figures/highway-layers.pdf\" width=\"90%\"> [Srivastava]\n",
    "1. aktualizacja wag wykorzystując SGD z momentum\n",
    "2. inicjalizacja\n",
    "  * inicjalizacja $w_T, b_T$ __niezależna__ od inicjalizacji $w,b$\n",
    "  * $b_T$ inicjalizowane na nieduże wartosci ujemne rzędu $-1 \\ldots -10$\n",
    "    * to będzie powodować na początku uczenia zachowanie w kierunku czynnika przeniesienia (carry)\n",
    "      * z początku większość informacji jest kopiowana\n",
    "      * analogiczne do sugestii inicjalizacji bramki zapominania w sieciach LSTM\n",
    "<img src=\"nn_figures/highway-weights.pdf\" width=\"90%\"> [Srivastava]\n",
    "2. najlepsze 50 sieci dla MNIST i CIFAR-100\n",
    "  * zerowa warstwa jest zwykła i zmniejsza wymiar do stałego wymiaru 50\n",
    "    * kolejno: bias bramki transformacji, średnie wyjście z bramki transformacji (dla 10 tysiecy przykładów), \n",
    "    * i dalej: przykładowe wyjscie dla bramki transformacji i całkowite wyjście dla losowego przykładu\n",
    "  * biasy bramki transformacji __rosną__ wraz z głębokością\n",
    "* średnie wyjścia bramek transformacji __zanikają__ wraz z głębokością\n",
    "  * czynnik przeniesienia staje się istotniejszy wraz z odległoscią od wejścia\n",
    "  \n",
    "3. istotność warstw w modelu\n",
    "\n",
    "<img src=\"nn_figures/highway-warstwy.pdf\" width=\"80%\"> [Srivastava]\n",
    "  * jaka jest istotność warstw w modelu?\n",
    "  * uszkodzenie pojeynczej warstwy przez ustawienie bramki transformacji pojedynczej warstwy na 0\n",
    "    * wtedy sieć kopiuje wejście\n",
    "    * dla problemu MNIST wyraźnie widać, że od pewnego momentu problem jest już praktycznie rozwiązany\n",
    "      * pod koniec jest pewna strata, wynikająca zapewne z konieczności zsumowania wyników (ostatnia wyjściowa warstwa to softmax)\n",
    "    * CIFAR-100 jest znacznie trudniejszy, co wyraźnie widać\n",
    "  * to pokazuje, że nadmiar warstw __nie jest__ problemem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci ResNet -- Residual Networks\n",
    "\n",
    "<img src=\"nn_figures/resnet-block.pdf\" width=\"60%\"> [He et al.] (każda 'layer' oznacza na rysunku operacje, np. pierwsza operację liniową, a druga funkcję aktywacji)\n",
    "1. jeśli aktywacja jest obliczana jako $$y=F(x)+x,$$ to gradient będzie się cofał do wejścia __bez zanikania__\n",
    "  * $f(x)$ jest dowolną operacją typu mnożenia macierzy, batch norm, konwolucji, etc.\n",
    "  * $f(x)$ może być całym __blokiem__ operacji\n",
    "\n",
    "<img src=\"nn_figures/resnet.png\" width=\"70%\"> [He et al.]\n",
    "\n",
    "2. podstawowym celem jest \n",
    "  * uczenie $F(x)$ jako __wartości rezydualnej__ ze względu na funkcję identyczności\n",
    "    * jeśli założymy, że wiele nieliniowych warstw jest w stanie aproksymować złożone funkcje, to __równoważnym__ jest, że są w stanie aproksymować funkcje rezydualne $H(x)-x$\n",
    "      * tu trzeba założyć, że $H(x)$ oraz $x$ mają ten sam wymiar\n",
    "    * zamiast aproksymować $H(x)$, wymagamy explicite by aproksymować $F(x)=H(x)-x$, stąd $H(x)=F(x)+x$\n",
    "    * rzadko identyczność jest optymalna, jednak ułatwia to uczenie\n",
    "      * metoda uczenia residuów jest wykorzystywana w innych modelach (drzewa gradientowe, algorytmy uczenia szeregów czasowych, etc.)\n",
    "  * jeśli identyczność jest optymalna, to łatwo ustawić wagi jako $0$\n",
    "  * jeśli optymalne mapowanie jest bliskie identyczności, to łatwiej znaleźć małe zmiany\n",
    "  * głębsze sieci resnet __z reguły__ dają lepsze wyniki\n",
    "\n",
    "<img src=\"nn_figures/resnet-imagenets.pdf\" width=\"80%\"> [He et al.]\n",
    "\n",
    "<img src=\"nn_figures/resnet-imagenets-graph.pdf\" width=\"90%\"> [He et al.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bardzo gładka propagacja wprzód\n",
    "1. obliczanie aktywacji wiele warstw wprzód jest bardzo proste\n",
    "$$\\begin{align}\n",
    "x_{l+1}&=x_l+F(x_l)\\\\\n",
    "x_{l+2}&=x_{l+1}+F(x_{l+1})=x_l+F(x_l)+F(x_{l+1})\\\\\n",
    "\\dots&\\dots\\\\\n",
    "x_L&=x_l+\\sum_{k=l}^{L-1}F(x_k)\n",
    "\\end{align}$$\n",
    "  * każde $x_l$ jest bezpośrednio przekazywane do $x_L$ __plus _residuum___ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bardzo gładka propagacja wstecz\n",
    "\n",
    "<img src=\"nn_figures/resnet-net.pdf\" width=\"30%\" align=\"left\"> [He et al.]\n",
    "\n",
    "1. wsteczna propagacja okazuje się też prosta\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E}{\\partial x_l}&=\\frac{\\partial E}{\\partial x_L}\\frac{\\partial x_L}{\\partial x_l}\\\\\n",
    "&=\\frac{\\partial E}{\\partial x_L}\\left(1+\\frac{\\partial}{\\partial x_l}\\sum_{k=l}^{L-1}F(x_k)\\right)\n",
    "\\end{align}$$\n",
    "2. każde $\\frac{\\partial E}{\\partial x_L}$ można przedstawić jako wsteczną propagację do dowolnego $\\frac{\\partial E}{\\partial x_l}$ __plus residuum__\n",
    "  * ponieważ $\\frac{\\partial E}{\\partial x_l}$ jest addytywne, to nie będzie zanikać gradient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inne bloki w ResNet\n",
    "<img src=\"nn_figures/resnet-two-blocks.pdf\" width=\"70%\"> [He et al.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask RCNN\n",
    "<img src=\"nn_figures/mask-rcnn.pdf\" width=\"70%\" align=\"center\"> [He et al.]\n",
    "\n",
    "1. Region Proposal Network RPN wyszukuje obszary potencjalnie zawierające obiekty\n",
    "2. w drugim etapie MASK-RCNN\n",
    "  * przeewiduje klasę\n",
    "  * bounding box\n",
    "  * maskę\n",
    "<img src=\"nn_figures/mask-rcnn-result.pdf\" width=\"90%\"> [He et al.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet\n",
    "\n",
    "<img src=\"nn_figures/densenet-architecture.png\" width=\"80%\">\n",
    "1. idea rozszerzająca poprzednie przez połączenie z __wszystkimi__ warstwami poprzednimi $$y=f(x, x_{-1}, x_{-2},\\dots)$$\n",
    "  * dla sieci konwolucyjnych muszą być zachowane odpowiednie warunki dotyczące wymiaru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci rezydualne z losowo usuwanymi warstwami (Huang et al.)\n",
    "\n",
    "<img src=\"nn_figures/highway-warstwy.pdf\" width=\"80%\"> [Srivastava]\n",
    "\n",
    "0. Uczenie bardzo głębokich sieci napotyka na wiele problemów\n",
    "  * zanikające gradienty\n",
    "    * wielokrotne wymnażanie (lub konwolucje) przy małych wagach powoduje brak efektywnosci uczenia wag bliskich wejscia\n",
    "    * potrzebne dobra inicjalizacja, nadzorowane uczenie warstw pośrednich (patrz Inception), Batch Normalization\n",
    "  * zanikające uzycie cech\n",
    "    * cechy wejściowe lub wcześnie wyliczone zanikają wskutek wymnażania wag\n",
    "    * są trudne do uzycia później\n",
    "    * konieczne połączenia skrótowe między wielu warstwami (patrz DenseNet)\n",
    "  *  uczenie może być bardzo czasochłonne ze względu na bardzo dużą liczbę parametrów\n",
    "  * dla wielu problemów wystarczająca jest mniejsza liczba warstw\n",
    "2. A może by część warstw usunąć?\n",
    "  * określić szansę __istotności__ warstwy w trakcie uczenia\n",
    "  * losowo warstwy redukować do identyczności\n",
    "  * przy inferencji uzywać wszystkich\n",
    "3. niech $r\\sim Bernoulli$\n",
    "  $$H_k(x_{k-1}) = ReLU(r_k F_k(x_{k-1}) + x_{k-1})$$\n",
    "  * dla $r_k=1$ mamy zwykły blok rezydualny\n",
    "  $$H_k(x_{k-1}) = ReLU(F_k(x_{k-1}) + x_{k-1})$$\n",
    "  * dla $r_k=0$ tylko przeniesienie\n",
    "  $$H_k(x_{k-1}) = ReLU(x_{k-1})$$\n",
    "    $$x_{k-1}=H_{k-1}(x_{k-2})=ReLU(r_{k-1} F_k(x_{k-2}) + x_{k-2})$$\n",
    "    stąd\n",
    "    $$\\begin{align}\n",
    "    H_k(x_{k-1})=ReLU(x_{k-1})&=ReLU(ReLU(r_{k-1} F_k(x_{k-2}) + x_{k-2}))\\\\\n",
    "    &=ReLU(r_{k-1} F_k(x_{k-2}) + x_{k-2})\\\\\n",
    "    &=x_{k-1}\\\\\n",
    "    &=H_{k-1}(x_{k-1})\n",
    "    \\end{align}$$\n",
    "4. Autorzy proponują używać __prawdopodobieństwa przeżycia__ $p_k$ dla rozkładu Bernoulliego zmniejszanego liniowo wraz z numerem warstwy\n",
    "$$p_k=1-\\frac{k}{K}(1-p_K)$$\n",
    "\n",
    "  <img src=\"nn_figures/resnet-stochastic.pdf\" width=\"80%\"> [Huang et al.]\n",
    "\n",
    "  * im dalej od wejscia, tym większa szansa na opuszczenie\n",
    "    * zgodne z obserwacjami w sieciach Highway\n",
    "  * warstwy opuszczane dla każdego mini-batcha\n",
    "  * autorzy używają $p_K=0.5$\n",
    "    * \n",
    "    * uczenie okazuje się stabilne\n",
    "  * możliwe jest użycie stałego $p_k=p_K$\n",
    "    * zwykle większe przyspieszenie (czemu???)\n",
    "    * jednak zdecydowanie słabsze wyniki\n",
    "      * dopiero wysoka wartość $p_K$ daje znaczący spadek błędu testowania, jednak porównywalny dla tego z liniowym wzrostem $p_k$ dla znacznie mnieszego $p_K$\n",
    "    * $p_K=0.5$ daje najlepsze wyniki dla reguły liniowej\n",
    "    * a czy możliwe jest __adaptacyjne__ modyfikowanie $p_k$???\n",
    "      * warstwy blisko końcowej mają większe znaczenie\n",
    "      * rozpocząć do reguły liniowej\n",
    "      * co ustaloną liczbę kroków wykonywać test amputacji warstw\n",
    "      * odpowiednio zwiększać/zmniejszać $p_k$\n",
    "5. Inferencja\n",
    "  * w trakcie poszczególne $F_k()$ są wykorzystywane __losowo__ proporcjonalnie do $p_k$\n",
    "  * inferencja wymaga przewtorzenia \n",
    "  $$\\widehat{H}_k(\\widehat{x}_{k-1}) = ReLU(p_k F_k(\\widehat{x}_{k-1}) + \\widehat{x}_{k-1})$$\n",
    "5. efekty?\n",
    "  * lepsze wyniki \n",
    "    * błąd na zbiorze trenującym jest __wyższy__ niż ten dla pełnej sieci\n",
    "    * bład na zbiorze testujacym jest __niższy__ dając jedne z [najlepszych znanych wyników](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)\n",
    "      * dla CIFAR-10 $5.25\\%$ o ponad $1\\%$ (ResNet-110)\n",
    "      * dla CIFAR-100 $24.98\\%$ o prawie $3\\%$ (ResNet-110)\n",
    "  * efekty dla ekstremalnie głębokich sieci\n",
    "    * ResNet o głębokości $1202$ warstw dawał wyniki __słabsze__ niż ResNet-110 dla CIFAR-10\n",
    "      * overfitting\n",
    "    * stochastyczne usuwanie warstw na takiej sieci dało __spadek__ błędu ($4.91\\%$, spadek o ok. $0.3\\%$)\n",
    "  * znaczne przyspieszenie uczenia\n",
    "    * czas uczenia jest proporcjonalny do __oczekiwanej__ głębokości sieci, tzn. liczby pozostawionych warstw\n",
    "      $$\\mathbb{E}(\\overline{K})=\\sum_{k=1}^Kp_k$$\n",
    "    * dla $p_K=0.5$ mamy $$\\mathbb{E}(\\overline{K})\\simeq3/4K$$\n",
    "  * dobry wpływ na wielkość gradientu\n",
    "    * norma gradientu sieci stochastycznej była stale większa od normy dla sieci o stałej długości\n",
    "  * efekty przypominając Dropout\n",
    "    * jednak usuwane są __warstwy__, nie pojedyncze neurony\n",
    "    * generowanych jest losowych $2^K$ sieci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
