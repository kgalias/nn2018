{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big>Sieci rekurencyjne</big></big></big></big></big>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<id=tocheading><big><big><big><big>Spis treści</big></big></big></big>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci rekurencyjne\n",
    "<img src=\"nn_figures/rnn.jpeg\" width=\"90%\"> [Nature]\n",
    "\n",
    "1. __rozwinięcie sieci__ w $n$ składowych dla wygenerowania $n$-elementowego ciągu\n",
    "2. $x_t$ to wejscie w chwili $t$\n",
    "  * gdy generujemy więcej niż jedno słowo (znak) wprzód, to poprzednio wygenerowane staje się wejściem do następnego\n",
    "3. __pamięć__ $s_t$ \n",
    "  * obliczane na podstawie poprzednich $s_t=f(Ux_t+Ws_{t-1})$\n",
    "  * $f$ funkcją nieliniową\n",
    "4. __wyjście__ $o_t$ w chwili $t$\n",
    "  * zwykle jako $softmax(Vs_t)$\n",
    "  * zwraca wektor prawdopodobieństw stanów dyskretnych\n",
    "  * interesujący może być np. tylko ostatni stan określający znaczenie zdania (sentiment analysis)\n",
    "5. $s_t$ przechowuje __całą__ informację na temat poprzednich stanów obliczeń\n",
    "  * praktycznie nie jest wystarczająca\n",
    "6. __wszystkie__ kroki __dzielą__ te same parametry $U, V, W$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemy RNN\n",
    "1. od dawna znane różne podstawowe architektury RNN\n",
    "  * stanem pamięci jest stan ukryty i tam następuje rekurencja\n",
    "  * aktualny stan wyjsciowy staje się _dodatkowym_ stanem wejściowym (jak w automatach)\n",
    "2. podstawowymi problemami są\n",
    "  * pamięć jedynie ostatnich akcji, _zapominanie_ stanów poprzednich\n",
    "  * pamięć jedynie pojedynczych stanów globalnych dla całego modelu bez pamięci stanów ostatnich\n",
    "  * stąd potrzeba modelu wypełniającego tą dziurę - __long-short time memory__\n",
    "  * eksplodujące / zanikające gradienty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zastosowania\n",
    "1. __modelowanie i generowanie języka__\n",
    "  * predykcja __prawdopodobieństwa__, że zdanie jest poprawne\n",
    "  * samplując z tego dostajemy model __generatywny__\n",
    "  * model językowy z użyciem __n-gramów__\n",
    "  $$P(w_1,\\dots,w_m)=\\prod_{i=1}^mP(w_i\\mid w_1,\\dots,w_{i-1})\\approx\\prod_{i=1}^mP(w_i\\mid w_{i-(n-1)},\\dots,w_{i-1})$$\n",
    "  dla n-gramów $$P(w_i\\mid w_{i-(n-1)},\\dots,w_{i-1})=\\frac{\\#(w_{i-(n-1)},\\dots,w_{i-1}, w_i)}{\\#(w_{i-(n-1)},\\dots,w_{i-1})}$$\n",
    "2. __tłumaczenie języka__\n",
    "  * podobne do modelowania\n",
    "  * wymaga zwykle przeczytania kompletnego zdania w jednym języku __przed__ wygenerowaniem pierwszego słowa nowego zdania\n",
    "3. __rozpoznawanie języka__\n",
    "  * wejściem są odczytane __fonemy__\n",
    "  * wyjściem nowe fonemy lub transkrypcja na zdania (tłumaczenie)\n",
    "4. Modele RNN pozwalają przyjmować wejścia o __zmiennej długości__\n",
    "  * na przykład opis obrazu jako wiele losowych sampli z niego\n",
    "\n",
    "<img src=\"nn_figures/rnn-diagrams.jpeg\" width=\"80%\"> [Karpathy]\n",
    "* od lewej do prawej\n",
    "  * zwykłe przetwarzanie \n",
    "  * tłumaczenie pojedynczego obiektu na opis (na przykład obraz na wiele związanych znim słów)\n",
    "  * __analiza sentymentu__ przyjmuje całą sekwencję i ocenia ją na końcu\n",
    "  * tłumczaenie maszynowe\n",
    "  * równoległe wejście-wyjście"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-Propagation Through Time BPTT\n",
    "1. za każdym razem patrzymy kilka kroków wstecz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# klasa RNN (za http://wildml.com)\n",
    "class RNNNumpy():\n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), \n",
    "                                   (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), \n",
    "                                   (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), \n",
    "                                   (hidden_dim, hidden_dim))\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    " \n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    " \n",
    "    #RNNNumpy.predict = predict\n",
    "    #RNNNumpy.forward_propagation = forward_propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT\n",
    "<img src=\"nn_figures/rnn.jpeg\" width=\"70%\"> [Nature]\n",
    "1. w każdym kroku należy znaleźć wszystkie macierze parametrów $U, V, W$\n",
    "  * są wspólne dla wszystkich kroków\n",
    "  * zwykle mają dużo parametrów\n",
    "  * niech będzie $N$ różnych słów, a pamięć jest reprezentowana przez wektor o długosci $K$\n",
    "    * $x_t\\in\\mathbb{R}^{N}$\n",
    "    * $o_t\\in\\mathbb{R}^{N}$\n",
    "    * $s_t\\in\\mathbb{R}^{K}$\n",
    "    * $U\\in\\mathbb{R}^{K\\times{}N}$\n",
    "    * $V\\in\\mathbb{R}^{N\\times{}K}$\n",
    "    * $W\\in\\mathbb{R}^{K\\times{}K}$\n",
    "2. parametry są __dzielone__ we wszystkich przewidywanych krokach\n",
    "  * gradient w aktualnym kroku zależy \n",
    "    * od obliczeń w aktualnym kroku czasu\n",
    "    * od obliczeń w poprzednim kroku\n",
    "  * odpowiada to wykorzystaniu __reguły łańcuchowej__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# za [Britz]\n",
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # wykonanie propagacji wprzód (zwraca ostatnie wyjscie i stan pamięci)\n",
    "    #  forward_propagation() wykonuje kroki wprzód zapamiętując wszystkie wartosci pośrednie,\n",
    "    #  które będą później potrzebne\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # macierze potrzebne dla akumulacji gradientów\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # teraz cofając się wstecz w obliczeniach\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # wstęczne obliczenia dla ostatniego kroku\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # wsteczna propagacja w czasie po poprzedzających krokach, ale co najwyżej bptt_truncate kroków\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # aktualizacja\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT\n",
    "1. Algorytm jest w stanie nauczyć się prostych zależności\n",
    "  * kolejność słów: bi-gramy, tri-gramy\n",
    "  * częstość występowania słów\n",
    "  * prostej składni\n",
    "  * prostej interpunkcji\n",
    "3. Jednak\n",
    "  *\n",
    "  * podawane zdania są zbyt krótkie by nauczyć poprawnej gramatyki\n",
    "  * dłuższe zdania znacznie zwiększają złożoność uczenia\n",
    "  * __nie jest w stanie__ nauczyć się zależności między __odległymi__ słowami\n",
    "    * proste RNN są w stanie imitować __jedynie__ pamięć krótko-terminową\n",
    "  * BPTT cierpi w dużym stopniu na problem zanikającego / eksplodującego gradientu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPTT koszt i wsteczna propagacja\n",
    "<img src=\"nn_figures/rnn.jpeg\" width=\"70%\"> [Nature]\n",
    "\n",
    "<img src=\"nn_figures/rnn-bptt1.png\" width=\"70%\"> [Nature]\n",
    "\n",
    "1. koszt\n",
    "$$E(y, \\widehat{y})=\\sum_tE_t(y_t,\\widehat{y}_t)$$\n",
    "2. dla $z_3=Vs_3$ mamy\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E_3}{\\partial V} &= \\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial\\widehat{y}_3}{\\partial V}\\\\\n",
    "&=\\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial\\widehat{y}_3}{\\partial z_3} \\frac{\\partial z_3}{\\partial V}\\\\\n",
    "&=(\\widehat{y}_3-y_3)\\otimes s_3\n",
    "\\end{align}$$\n",
    "3. dla pochodnej po $W$ zaczyna się pojawiać rekurencja\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E_3}{\\partial W} &= \\frac{\\partial E_3}{\\partial s_3} \\frac{\\partial s_3}{\\partial W}\\\\\n",
    "&= \\frac{\\partial E_3}{\\partial \\widehat{y}_3}\\frac{\\partial \\widehat{y}_3}{\\partial s_3} \\frac{\\partial s_3}{\\partial W}\\\\\n",
    "&\\hskip3em\\text{jednak $s_3$ bezpośrednio zależy od $s_2$, które nie jest stałe!}\\\\\n",
    "s_3&=\\tanh(U x_t+W s_2)\\\\\n",
    "\\frac{\\partial E_3}{\\partial W} &=\\sum_{t=0}^3\\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial \\widehat{y}_3}{\\partial s_3} \\frac{\\partial s_3}{\\partial s_t}\\frac{\\partial s_t}{\\partial W}\\\\\n",
    "\\end{align}$$\n",
    "<img src=\"nn_figures/rnn-bptt-gradients.png\" width=\"70%\"> [Nature]\n",
    "4. w rzeczywistości BPTT niewiele się różni od zwykłej wstecznej propagacji\n",
    "  * w sieci warstwowej parametry między warstwami __nie są__ dzielone\n",
    "  * nie ma potrzeby ich sumowania\n",
    "  * w analogiczny sposób można zdefiniować regułę delta\n",
    "  $$\\delta^3_2=\\frac{\\partial E_3}{\\partial z_2}=\\frac{\\partial E_3}{\\partial s_3}\\frac{\\partial s_3}{\\partial s_2}\\frac{\\partial s_2}{\\partial s_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPTT i zanikający gradient\n",
    "1. podstawowym problemem w uczeniu jest zanikanie gradientu\n",
    "  * problem zauważył Hochreiter, który był autorem modelu LSTM\n",
    "$$\\frac{\\partial E_3}{\\partial W} =\\sum_{t=0}^3\\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial \\widehat{y}_3}{\\partial s_3} \\frac{\\partial s_3}{\\partial s_t}\\frac{\\partial s_t}{\\partial W}$$\n",
    "2. w rozwiązaniu występuje czynnik\n",
    "$$\\frac{\\partial s_3}{\\partial s_t}$$\n",
    "  * i tak chociażby $$\\frac{\\partial s_3}{\\partial s_1} = \\frac{\\partial s_3}{\\partial s_2}\\frac{\\partial s_2}{\\partial s_1}$$\n",
    "  * skąd mamy\n",
    "  $$\\frac{\\partial E_3}{\\partial W} =\\sum_{t=0}^3\\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial \\widehat{y}_3}{\\partial s_3} \\left(\\prod_{j=t+1}\\frac{\\partial s_j}{\\partial s_{j-1}}\\right)\\frac{\\partial s_t}{\\partial W}$$\n",
    "  * $s_t=\\tanh(Ux_t+Ws{t-1})$\n",
    "  * $\\tanh()$ ma obszar saturacji po lewej i prawej stronie, a jego gradient maleje __eksponencjalnie__ szybko\n",
    "  * jeśli aktywacje są daleko od własciwych, to gradient spada prawie do zera\n",
    "  * wymnażanie bardzo małych wartosci tylko eksponencjalnie szybko je jeszcze zmniejsza...\n",
    "3. eksplodujący gradient pojawia się równie często\n",
    "  * jest efektem kilku wysokich aktywacji\n",
    "  * może prowadzić do oscylacji, gdy nadchodzące sygnały są sprzeczne\n",
    "  * w miarę łatwo sobie z nim poradzić przez obcinanie gradientu z wysoką normą\n",
    "4. a jak z zanikającym gradientem?\n",
    "  * trudniej: sieć nie uczy się wale albo potrzebuje wykładniczo wiele czasu\n",
    "  * poprawna inicjalizacja\n",
    "  * ReLU zamiast funkcji sigmoidalnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemy\n",
    "1. __krótka__ a __długa__ pamięć\n",
    "  * RNN z algorytmem typu BPTT szybko ___zapomina___ informacje\n",
    "  * korzysta tylko z ostatniej\n",
    "  * model dla angielskiego na poziomie znaków szybko nauczy się, że po znaku `q` __zawsze__ występuje znak `u`\n",
    "  * jednak nie nauczy się informacji kontekstowej z poprzedniego zdania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Time Memory LSTM\n",
    "1. w 1991 Sepp Hochreiter obronił pracę dyplomową w Monachium w której \n",
    "  * przedstawił szczegółową analizę uczenia sieci rekurencyjnych\n",
    "  * odkrył zjawisko zanikajacego i eksplodujacego gradientu\n",
    "2. w 1997 zaproponował, wraz z Jurgenem Schmidhuberem, model LSTM \n",
    "  * Neural Computation:9(8):1735-1780 \n",
    "  * wcześniej w 1995 w _technical document_ w Monachium\n",
    "3. model LSTM stał się początkiem dla wielu innych modeli\n",
    "  * rozwinięcia LSTM, np. GRU (dodatkowe bramki, inny przepływ, etc.)\n",
    "  * sieci warstwowe typu Highway\n",
    "  * sieci ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (za Hochreiterem)\n",
    "1. jako podstawowy problem Hochreiter zauważył ___zanikający sygnał błędu___\n",
    "  * błąd zanika i sieć nie uczy się niczego w sensownym czasie\n",
    "  * dla sigmoidalnych funkcji aktywacji wagi musiałyby być większe od $\\approx4$ by sygnał miał wystarczającą wartość\n",
    "  * jednak większych przy inicjalizacji wagi nic nie pomogą, bo odpowiednia pochodna maleje jeszcze szybciej\n",
    "  * BPTT jest bardzo czuły na ostatnie zmiany/sygnały\n",
    "  * Hochreiter zauważył, że konieczne jest zapewnienienie stałego przepływu sygnału błędu\n",
    "    * wniosek: funkcja aktywacji __musi być liniowa__\n",
    "2. komórka LSTM\n",
    "\n",
    "<img src=\"nn_figures/LSTM-Hochreiter.pdf\" width=\"80%\"> [Neural Computation]\n",
    "\n",
    "<img src=\"nn_figures/gers_lstm.png\" width=\"80%\"> [Hochreiter]\n",
    "\n",
    "  * dodaje (multiplikatywną) __bramkę wejściową__ $in$, która ma zabezpieczyć zawartość pamięci od wpływu _nieistotnych_ wejść\n",
    "  * analogicznie __bramkę wyjściową__ $out$ mającą zabezpieczyć inne komórki przed wpływem (aktualnych) nieistotnych informacji w komórce\n",
    "  * __czemu bramki ?__\n",
    "    * bramka wejsciowa $in$ kontroluje przepływ sygnału błędu by zabezpieczyć przed konfliktami wag\n",
    "      * czasem komórka _powinna_ uzyć wejścia z innej komórki\n",
    "      * czasem nie\n",
    "      * bramka wejsciowa kontroluje to\n",
    "    * podobnie bramka $out$ kontroluje wagi wyjściowe\n",
    "    * bramki wejściowa/wyjściowa muszą się __nauczyć__, które sygnały wyłapać/zablokować"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM przykład (za Hochreiterem)\n",
    "<img src=\"nn_figures/LSTM-Hochreiter-flow.pdf\" width=\"80%\"> [Neural Computation]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM krok po kroku (za Christopher Olah)\n",
    "### bramka wejściowa\n",
    "\n",
    "<img src=\"nn_figures/LSTM3-focus-f.png\" width=\"80%\"> [Colah]\n",
    "  * sigmoidalna bramka wejsciowa decyduje __które__ informacje będą aktualizowane\n",
    "    * wartość poprzedniej pamieci jest __wymnażana__ przez $f_t$\n",
    "    * $f_t$ jest bramką _zapominającą_ nieistotne w tej chwili informacje\n",
    "\n",
    "### aktualizacja stanu\n",
    "<img src=\"nn_figures/LSTM3-focus-i.png\" width=\"80%\"> [Colah]\n",
    "  * komórka decyduje które wartości z wejścia należy _dodać_ do tej aktualizowanej\n",
    "    * wartości $i_t\\widetilde{C}_t$ są dodawane do poprzednio wyczyszczonej i odpowiednio przeskalowane\n",
    "    * to decyduje, które informacje z wejścia (i stanu pamięci) należy teraz użyć\n",
    "  * te wartości połączone decydują o wyjsciu stanu komórki\n",
    "\n",
    "\n",
    "<img src=\"nn_figures/LSTM3-focus-C.png\" width=\"80%\"> [Colah]\n",
    "\n",
    "### wyjście\n",
    "\n",
    "<img src=\"nn_figures/LSTM3-focus-o.png\" width=\"80%\"> [Colah]\n",
    "  * po pierwsze bramka __wyjściowa__ (sigmoidalna) decydująca, które elementy stanu należy przekazać na wyjście $o_t$\n",
    "  * stan komórki jest reskalowany do $(-1,+1)$ przez $\\tanh$\n",
    "  * przeskalowany stan komórki jest filtrowany przez bramkę wyjsciową\n",
    "  \n",
    "### warianty\n",
    "* bramki wykorzystują wgląd w stan komórki\n",
    "\n",
    "<img src=\"nn_figures/LSTM3-var-peepholes.png\" width=\"80%\"> [Colah]\n",
    "  * nie zawsze wykorzystywane przez wszystkie bramki\n",
    "\n",
    "* połączenie bramek zapominającej i wejsciowej\n",
    "\n",
    "<img src=\"nn_figures/LSTM3-var-tied.png\" width=\"80%\"> [Colah]\n",
    "  * zapomina jedynie te składniki, w które zostanie wstawiona nowa informacja\n",
    "\n",
    "* Gated Recurrent Unit (Cho, 2014)\n",
    "\n",
    "<img src=\"nn_figures/LSTM3-var-GRU.png\" width=\"80%\"> [Colah]\n",
    "  * spore uproszczenie, a przez to staje się popularna\n",
    "  * połączenie bramek zapiminajacej i wejsciowej\n",
    "  * łączy stan komórki $C_t$ wraz ze stanem ukrytym komórki $h_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jakie parametry?\n",
    "1. uczenie sieci LSTM nie jest trudne\n",
    "  * jest bardzo wiele wariantów\n",
    "  * architektura nie jest oczywista i ma wiele składników możliwe, że na wiarę\n",
    "  * które są najlepsze i jak je uczyć?\n",
    "2. R. Jozefowicz, Zaremba i Sutskever wykonali duży przegląd możliwych architektur wyciągając szereg wniosków\n",
    "  * istnieje wiele architektur podobnych do GRU lepszych na wielu zadaniach\n",
    "  * LSTM: jest generalnie najlepszy jeśli użyty był Dropout\n",
    "  * LSTM: dodawanie jedynki do bramki zapominającej zwykle minimalizuje różnicę do najlepszych architektur\n",
    "    * typowa inicjalizacja ustawia wszystkie wagi na małe wartości\n",
    "    * inicjalizacja biasu bramki zapominajacej na małą wartość sprzyja pojawieniu się zanikającego gradientu\n",
    "    * rozwiązaniem może być inicjalizacja go na wyższe wartości rzędu $1 - 2$\n",
    "  * istotność bramek\n",
    "    * bramka wejściowa __jest__ istotna\n",
    "    * bramka wyjściowa __nie jest__ istotna\n",
    "    * bramka zapominająca __jest bardzo__ istotna dla wszystkich problemów __poza__ modelowaniem języka\n",
    "3. autorzy wykorzystali prostą procedurę przeszukiwania\n",
    "  * utrzymywali listę 100 znalezionych najlepszych architektur poszukując dla nich najlepszych hiperparametrów\n",
    "  * w każdym etapie wykonuje jeden z kroków\n",
    "    * losuje jedną ze 100 architektur\n",
    "      * ewaluuje 20 losowych ustawień hiperparametrów dla każdego z 3 zadań\n",
    "      * ocenia przez\n",
    "      $$\\min\\frac{\\text{najlepsza dokładność dla architektury dla zadania}}{\\text{najlepszy wynik GRU dla zadania}}$$\n",
    "      przy czym GRU były wyliczone dla wszystkich dozwolonych architektur\n",
    "    * wybiera jedną ze 100 architektur\n",
    "      * mutuje parametry\n",
    "  * autorzy\n",
    "    * ewaluowali 10 tysięcy różnych architektur\n",
    "    * 1000 z nich przeszło początkowy test zapamiętywania\n",
    "      * 5 znaków w sekwencji, dla wszystkich 26 możliwości\n",
    "      * jest czytanych w sekwencji\n",
    "      * i ma być odtworzone w tej samej sekwencji\n",
    "    * te 1000 architektur było sprawdzone dla średnio 2200 konfiguracji\n",
    "    * razem wykonali testy dla ok. 230 tysiecy konfiguracji\n",
    "    * zadania\n",
    "      * obliczenie sumy (w postaci znaków) dla sekwencji składajacej się z dwóch sekwencji\n",
    "      * predykcja następnego znaku w kodzie XML\n",
    "      * modelowanie języka Penn Tree-Bank\n",
    "      * (dodatkowo) modelowanie muzyki polifonicznej\n",
    "      \n",
    "  * rezultaty\n",
    "    * GRU było lepsze od LSTM na wszystkich zadaniach poza modelowaniem języka\n",
    "    * jeśli wykorzystano dropout, to LSTM było zdecydowanie najlepsze dla problemów modelowania jezyka\n",
    "    * LSTM z wysokim biasem bramki zapominajęcej było lepsze od innych LSTM i GRU na prawie wszystkich zadaniach\n",
    "    * trzy mutacje znalezione okazały się konkurencyjne do innych modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wielkość ukrytej warstwy\n",
    "1. $x_t$ i $h_t$ są wektorami\n",
    "  * jeśli rozpoznajemy znaki, to $h_t$ będzie wektorem prawdopodobieństw znaków\n",
    "  * w przypadku słów, trzeba wykorzystać jakiś word-embedding, by zredukować wymiarowość\n",
    "    * word-embedding pozwoli reprezentować słowa jako wektory w $\\mathbb{R}^K$\n",
    "    * word-embedding może być uczony __w trakcie__ uczenia modelu\n",
    "2. modele LSTM zawierają pojęcie __hidden layer__\n",
    "  * warstwa ukryta jest zbiorem neuronów w bramkach zapominajacych, wejsciowych, wyjsciowych, etc.\n",
    "  * ich wielkość to właśnie warstwa ukryta\n",
    "  * większa warstwa ukryta zapewnia modelowi większą pojemność\n",
    "3. innym typowym parametrem jest żądanie by model zwracał całe sekwencje\n",
    "  * w Keras to parametr `return_sequences`\n",
    " \n",
    "  <img src=\"nn_figures/Keras-LSTM-return-sequences.png\" width=\"80%\"> [Chollet]\n",
    "  * to pozwala budować sieci __wielopoziomowe__\n",
    "  <img src=\"nn_figures/LSTM-two-layer.png\" width=\"80%\"> [Colah]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
