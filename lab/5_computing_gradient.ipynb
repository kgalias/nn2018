{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing gradient\n",
    "\n",
    "In this notebook your task will be to implement forward and backward pass of Linear and backward pass of Conv1d modules.\n",
    "\n",
    "Resources:\n",
    "\n",
    "* Backprop with focus on PyTorch: https://www.youtube.com/watch?v=ma2KXWblllc (see also other lectures from this series)\n",
    "\n",
    "* Lecture on backpropagation https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/ (Lecture 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Get FashionMNIST (see 1b_FMNIST.ipynb for data exploration)\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Logistic regression needs 2D data\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# 0-1 normalization\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "# Convert to Torch Tensor. Just to avoid boilerplate code later\n",
    "x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "# Use only first 1k examples. Just for notebook to run faster\n",
    "x_train, y_train = x_train[0:1000], y_train[0:1000]\n",
    "x_test, y_test = x_test[0:1000], y_test[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from torch.autograd import gradcheck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear\n",
    "\n",
    "* Your task is to implement backward and forward pass of a Linear module. \n",
    "\n",
    "* **You cannot use for loops inside backward.**\n",
    "\n",
    "* Hint: try to implement first using for loops, and then transform to matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-3f6b2743eb2f>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-3f6b2743eb2f>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    grad_input = ??\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Linear(torch.autograd.Function):\n",
    "\n",
    "    def forward(self, input, weight, bias=None):\n",
    "        self.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        input, weight, bias = self.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        \n",
    "        # Hint: start with bias, use for loops if that's easier\n",
    "        if self.needs_input_grad[0]:\n",
    "            # Hint: grad_input should have same shape as input\n",
    "            grad_input = grad_output.mm(weight)\n",
    "            \n",
    "            # for-loop implementation\n",
    "#             grad_input = torch.zeros_like(input)\n",
    "#             for i in range(grad_input.shape[0]):\n",
    "#                 grad_input[i] = weight.t().matmul(grad_output[i])\n",
    "        \n",
    "        if self.needs_input_grad[1]:\n",
    "            # Hint: grad_weight should have same shape as weight\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "            \n",
    "            # for-loop implementation\n",
    "#             grad_weight = torch.zeros_like(weight)\n",
    "#             for i in range(grad_input.shape[0]):\n",
    "#                 grad_weight += grad_output[i].ger(input[i])\n",
    "            \n",
    "        if bias is not None and self.needs_input_grad[2]: \n",
    "            # Hint: grad_bias should have same shape as bias\n",
    "            grad_bias = grad_output.sum(dim=0)\n",
    "            \n",
    "            # for-loop implementation\n",
    "#             grad_bias = torch.zeros_like(bias)\n",
    "#             for i in range(grad_input.shape[0]):\n",
    "#                 grad_bias += grad_output[i] \n",
    "    \n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = (Variable(torch.randn(30,20).double(), requires_grad=True),  # x\n",
    "         Variable(torch.randn(15,20).double(), requires_grad=True),  # w\n",
    "         Variable(torch.randn(15,).double(), requires_grad=True))    # b\n",
    "test = gradcheck(Linear(), input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv1d\n",
    "\n",
    "Your task will be to implement part of the backward pass of 1d convolutional layer.\n",
    "\n",
    "We will have separate lab on convolutions. A crash course on CNNs:\n",
    "\n",
    "<img width=300 src=http://cs231n.github.io/assets/nn1/neural_net2.jpeg>\n",
    "\n",
    "<img width=400 src=http://cs231n.github.io/assets/cnn/stride.jpeg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of forward\n",
    "\n",
    "Note that test uses same input as\n",
    "\n",
    "<img width=400 src=http://cs231n.github.io/assets/cnn/stride.jpeg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       " -2  2  1  2  1\n",
       "\n",
       "(1 ,.,.) = \n",
       " -2  2  1 -4  1\n",
       "[torch.FloatTensor of size 2x1x5]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example with a single input channel\n",
    "\n",
    "# Input\n",
    "ex1 = [[0, 1, 2, -1, 1, -3, 0]]\n",
    "ex2 = [[0, 1, 2, -1, 1, 3, 0]]\n",
    "input = torch.autograd.Variable(torch.Tensor([ex1, ex2]))\n",
    "\n",
    "# Define conv1d\n",
    "weight = torch.autograd.Variable(torch.Tensor([[[1, 0, -1]]]) )\n",
    "bias = torch.autograd.Variable(torch.Tensor([0]))\n",
    "conv1d = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3)\n",
    "\n",
    "# Compute output. Note that ex1 is same as in the figure\n",
    "F.conv1d(input, weight=weight, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  2  5  5  0  1\n",
       "\n",
       "(1 ,.,.) = \n",
       "  2  5  5  0  7\n",
       "[torch.FloatTensor of size 2x1x5]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example with two input channels\n",
    "\n",
    "ex1 = [[0, 1, 2, -1, 1, -3, 0], [0, 1, 3, -1, 2, -3, 1]]\n",
    "ex2 = [[0, 1, 2, -1, 1, 3, 0], [0, 1, 3, -1, 2, 3, 1]]\n",
    "input = torch.autograd.Variable(torch.Tensor([ex1, ex2]))\n",
    "\n",
    "weight = torch.autograd.Variable(torch.Tensor([[[1, 0, -1], [1, 1, 1]]]) )\n",
    "bias = torch.autograd.Variable(torch.Tensor([0]))\n",
    "\n",
    "conv1d = torch.nn.Conv1d(in_channels=2, out_channels=1, kernel_size=3)\n",
    "\n",
    "F.conv1d(input, weight=weight, bias=bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement backward \n",
    "\n",
    "* Implement only gradient with respect to bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inherit from Function\n",
    "class Conv1D(torch.autograd.Function):\n",
    "    # bias is an optional argument\n",
    "    def forward(self, input, weight, bias=None):\n",
    "        input = input\n",
    "        self.save_for_backward(input, weight, bias)\n",
    "        output = F.conv1d(torch.autograd.Variable(input), \n",
    "                          weight=torch.autograd.Variable(weight), \n",
    "                          bias=torch.autograd.Variable(bias))\n",
    "        return output.data\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    def backward(self, grad_output):\n",
    "        input, weight, bias = self.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        \n",
    "        if self.needs_input_grad[0]:\n",
    "            grad_input = None\n",
    "        if self.needs_input_grad[1]:\n",
    "            grad_weight = None\n",
    "        if bias is not None and self.needs_input_grad[2]:\n",
    " local\n",
    "            # Hint: first write solution using for loop\n",
    "            grad_bias = grad_output.sum(dim=2).sum(dim=0)\n",
    "            \n",
    " remote\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = (Variable(torch.randn(2, 4, 5).double(), requires_grad=False), # 2 ex, 4 channels, 5 length\n",
    "         Variable(torch.randn(3, 4, 2).double(), requires_grad=False), # 3 out channels, 4 in channels\n",
    "         Variable(torch.randn(3).double(), requires_grad=True)) # 1 bias for each out channel\n",
    "test = gradcheck(Conv1D(), input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test linear\n",
    "input = (Variable(torch.randn(20,20).double(), requires_grad=True), \n",
    "         Variable(torch.randn(15,20).double(), requires_grad=True))\n",
    "result['linear'] = 0.5*int(gradcheck(Linear(), input, eps=1e-6, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test conv1d\n",
    "input = (Variable(torch.randn(2, 4, 5).double(), requires_grad=False), # 2 ex, 4 channels, 5 length\n",
    "         Variable(torch.randn(3, 4, 2).double(), requires_grad=False), # 3 out channels, 4 in channels\n",
    "         Variable(torch.randn(3).double(), requires_grad=True)) # 1 bias for each out channel\n",
    "result['conv1d'] = 0.5*int(gradcheck(Conv1D(), input, eps=1e-6, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1d': 0.5, 'linear': 0.5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(result, open(\"5_computing_gradient_backprop.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nn2018]",
   "language": "python",
   "name": "conda-env-nn2018-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
