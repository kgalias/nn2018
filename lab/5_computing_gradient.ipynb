{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing gradient\n",
    "\n",
    "In this notebook your task will be to implement forward and backward pass of Linear and backward pass of Conv1d modules.\n",
    "\n",
    "Resources:\n",
    "\n",
    "* Backprop with focus on PyTorch: https://www.youtube.com/watch?v=ma2KXWblllc (see also other lectures from this series)\n",
    "\n",
    "* Lecture on backpropagation https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/ (Lecture 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kg/miniconda3/envs/nn2018/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from torch.autograd import gradcheck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear\n",
    "\n",
    "* Your task is to implement backward and forward pass of a Linear module. \n",
    "\n",
    "* **You cannot use for loops inside backward.**\n",
    "\n",
    "* Hint: try to implement first using for loops, and then transform to matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(torch.autograd.Function):\n",
    "\n",
    "    def forward(self, input, weight, bias=None):\n",
    "        self.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        input, weight, bias = self.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        \n",
    "        # Hint: start with bias, use for loops if that's easier\n",
    "        if self.needs_input_grad[0]:\n",
    "            # Hint: grad_input should have same shape as input\n",
    "            grad_input = grad_output.mm(weight)\n",
    "            \n",
    "            # for-loop implementation\n",
    "#             grad_input = torch.zeros_like(input)\n",
    "#             for i in range(grad_input.shape[0]):\n",
    "#                 grad_input[i] = weight.t().matmul(grad_output[i])\n",
    "        \n",
    "        if self.needs_input_grad[1]:\n",
    "            # Hint: grad_weight should have same shape as weight\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "            \n",
    "            # for-loop implementation\n",
    "#             grad_weight = torch.zeros_like(weight)\n",
    "#             for i in range(grad_input.shape[0]):\n",
    "#                 grad_weight += grad_output[i].ger(input[i])\n",
    "            \n",
    "        if bias is not None and self.needs_input_grad[2]: \n",
    "            # Hint: grad_bias should have same shape as bias\n",
    "            grad_bias = grad_output.sum(dim=0)\n",
    "            \n",
    "            # for-loop implementation\n",
    "#             grad_bias = torch.zeros_like(bias)\n",
    "#             for i in range(grad_input.shape[0]):\n",
    "#                 grad_bias += grad_output[i] \n",
    "    \n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = (Variable(torch.randn(30,20).double(), requires_grad=True),  # x\n",
    "         Variable(torch.randn(15,20).double(), requires_grad=True),  # w\n",
    "         Variable(torch.randn(15,).double(), requires_grad=True))    # b\n",
    "test = gradcheck(Linear(), input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv1d\n",
    "\n",
    "Your task will be to implement part of the backward pass of 1d convolutional layer.\n",
    "\n",
    "We will have separate lab on convolutions. A crash course on CNNs:\n",
    "\n",
    "<img width=300 src=http://cs231n.github.io/assets/nn1/neural_net2.jpeg>\n",
    "\n",
    "<img width=400 src=http://cs231n.github.io/assets/cnn/stride.jpeg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of forward\n",
    "\n",
    "Note that test uses same input as\n",
    "\n",
    "<img width=400 src=http://cs231n.github.io/assets/cnn/stride.jpeg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       " -2  2  1  2  1\n",
       "\n",
       "(1 ,.,.) = \n",
       " -2  2  1 -4  1\n",
       "[torch.FloatTensor of size 2x1x5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example with a single input channel\n",
    "\n",
    "# Input\n",
    "ex1 = [[0, 1, 2, -1, 1, -3, 0]]\n",
    "ex2 = [[0, 1, 2, -1, 1, 3, 0]]\n",
    "input = torch.autograd.Variable(torch.Tensor([ex1, ex2]))\n",
    "\n",
    "# Define conv1d\n",
    "weight = torch.autograd.Variable(torch.Tensor([[[1, 0, -1]]]) )\n",
    "bias = torch.autograd.Variable(torch.Tensor([0]))\n",
    "conv1d = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3)\n",
    "\n",
    "# Compute output. Note that ex1 is same as in the figure\n",
    "F.conv1d(input, weight=weight, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  2  5  5  0  1\n",
       "\n",
       "(1 ,.,.) = \n",
       "  2  5  5  0  7\n",
       "[torch.FloatTensor of size 2x1x5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example with two input channels\n",
    "\n",
    "ex1 = [[0, 1, 2, -1, 1, -3, 0], [0, 1, 3, -1, 2, -3, 1]]\n",
    "ex2 = [[0, 1, 2, -1, 1, 3, 0], [0, 1, 3, -1, 2, 3, 1]]\n",
    "input = torch.autograd.Variable(torch.Tensor([ex1, ex2]))\n",
    "\n",
    "weight = torch.autograd.Variable(torch.Tensor([[[1, 0, -1], [1, 1, 1]]]) )\n",
    "bias = torch.autograd.Variable(torch.Tensor([0]))\n",
    "\n",
    "conv1d = torch.nn.Conv1d(in_channels=2, out_channels=1, kernel_size=3)\n",
    "\n",
    "F.conv1d(input, weight=weight, bias=bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement backward \n",
    "\n",
    "* Implement only gradient with respect to bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inherit from Function\n",
    "class Conv1D(torch.autograd.Function):\n",
    "    # bias is an optional argument\n",
    "    def forward(self, input, weight, bias=None):\n",
    "        input = input\n",
    "        self.save_for_backward(input, weight, bias)\n",
    "        output = F.conv1d(torch.autograd.Variable(input), \n",
    "                          weight=torch.autograd.Variable(weight), \n",
    "                          bias=torch.autograd.Variable(bias))\n",
    "        return output.data\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    def backward(self, grad_output):\n",
    "        input, weight, bias = self.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        \n",
    "        if self.needs_input_grad[0]:\n",
    "            # You don't have to implement this\n",
    "            grad_input = None\n",
    "        if self.needs_input_grad[1]:\n",
    "            # You don't have to implement this\n",
    "            grad_weight = None\n",
    "        if bias is not None and self.needs_input_grad[2]:\n",
    "            # Hint: first write solution using for loop\n",
    "            grad_bias = grad_output.sum(dim=2).sum(dim=0)\n",
    "            \n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       " -2  2  1  2  1\n",
       "\n",
       "(1 ,.,.) = \n",
       " -2  2  1 -4  1\n",
       "[torch.FloatTensor of size 2x1x5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex1 = [[0, 1, 2, -1, 1, -3, 0]]\n",
    "ex2 = [[0, 1, 2, -1, 1, 3, 0]]\n",
    "input = torch.autograd.Variable(torch.Tensor([ex1, ex2]))\n",
    "\n",
    "weight = torch.autograd.Variable(torch.Tensor([[[1, 0, -1]]]) )\n",
    "bias = torch.autograd.Variable(torch.Tensor([0]))\n",
    "\n",
    "torch.nn.functional.conv1d(input, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = (Variable(torch.randn(2, 4, 5).double(), requires_grad=False), # 2 ex, 4 channels, 5 length\n",
    "         Variable(torch.randn(3, 4, 2).double(), requires_grad=False), # 3 out channels, 4 in channels\n",
    "         Variable(torch.randn(3).double(), requires_grad=True)) # 1 bias for each out channel\n",
    "test = gradcheck(Conv1D(), input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test linear\n",
    "input = (Variable(torch.randn(20,20).double(), requires_grad=True), \n",
    "         Variable(torch.randn(15,20).double(), requires_grad=True))\n",
    "result['linear'] = 0.5*int(gradcheck(Linear(), input, eps=1e-6, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test conv1d\n",
    "input = (Variable(torch.randn(2, 4, 5).double(), requires_grad=False), # 2 ex, 4 channels, 5 length\n",
    "         Variable(torch.randn(3, 4, 2).double(), requires_grad=False), # 3 out channels, 4 in channels\n",
    "         Variable(torch.randn(3).double(), requires_grad=True)) # 1 bias for each out channel\n",
    "result['conv1d'] = 0.5*int(gradcheck(Conv1D(), input, eps=1e-6, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1d': 0.5, 'linear': 0.5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(result, open(\"5_computing_gradient_backprop.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nn2018]",
   "language": "python",
   "name": "conda-env-nn2018-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
