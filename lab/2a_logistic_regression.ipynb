{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial logistic regression in numpy\n",
    "\n",
    "In this notebook we will refresh our knowledge of linear models. This notebook assumes familiarity with basic machine learning concepts. See \"Resources\" section for material if you are unfamiliar with some of the concepts.\n",
    "\n",
    "Your tasks will be:\n",
    "\n",
    "* Implement binary logistic regression using numpy\n",
    "* Implement SGD training loop \n",
    "\n",
    "Goal is to:\n",
    "\n",
    "* Get accustomed with the environment and grading\n",
    "* Refresh our numpy skills\n",
    "* Refresh our machine learning knowledge\n",
    "\n",
    "Next up:\n",
    "\n",
    "* Implementing multinomial logistic regression in PyTorch\n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "If content of this notebook is new, confusing or difficult - don't worry! Check out the following resources:\n",
    "* Logistic regression:  https://www.coursera.org/learn/neural-networks-deep-learning/lecture/yWaRd/logistic-regression-cost-function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kg/miniconda3/envs/nn2018/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get FashionMNIST (see 1b_FMNIST.ipynb for data exploration)\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Logistic regression needs 2D data\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# 0-1 normalization\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "# For simplicity - implicit bias\n",
    "x_train = np.concatenate([np.ones_like(x_train[:, 0:1]), x_train], axis=1)\n",
    "x_test = np.concatenate([np.ones_like(x_test[:, 0:1]), x_test], axis=1)\n",
    "\n",
    "# Use only first 1k examples. Just for notebook to run faster\n",
    "x_train, y_train = x_train[0:1000], y_train[0:1000]\n",
    "x_test, y_test = x_test[0:1000], y_test[0:1000]\n",
    "\n",
    "# Cast as binary\n",
    "y_train = (y_train == 0).astype(\"int\")\n",
    "y_test = (y_test == 0).astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy primer\n",
    "\n",
    "This section introduces useful concepts from Numpy to complete the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/gmum/nn2018/raw/master/lab/fig/2/python-comic-2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  [1 2 3 4]\n",
      "Pierwszy element:  1\n",
      "Drugi element!:  2\n",
      "a+1: [2 3 4 5]\n",
      "2*a: [2 4 6 8]\n",
      "b:  [1 2 3 4]\n",
      "Iloczyn skalarny <a,b>:  30\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "print(\"a: \", a)\n",
    "print(\"Pierwszy element: \", a[0])\n",
    "print(\"Drugi element!: \", a[1])\n",
    "print(\"a+1:\", a + 1)\n",
    "print(\"2*a:\", 2*a)\n",
    "b = np.array([1, 2, 3, 4])\n",
    "print(\"b: \", b)\n",
    "print(\"Iloczyn skalarny <a,b>: \", np.inner(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "## Model\n",
    "\n",
    "\n",
    "Class prediction is given by:\n",
    "\n",
    "<br>\n",
    "<font size=4>\n",
    "$ p(\\textbf{y = 1} | \\textbf{x}, \\textbf{w}) = \\sigma(\\langle\\textbf{w}, \\textbf{x}\\rangle)$\n",
    "</font>\n",
    "\n",
    ", where $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$.\n",
    "\n",
    "http://www.deeplearningbook.org/, Ch.1.\n",
    "\n",
    "<img width=600 src=\"https://github.com/gmum/nn2018/raw/master/lab/fig/1/fig4.png\">\n",
    "\n",
    "\n",
    "Loss is given by the cross-entropy loss:\n",
    "\n",
    "<br>\n",
    "<font size=4>\n",
    "$ L(\\textbf{w}) = - \\frac{1}{N} (\\sum_i y_i \\log(p(\\textbf{y=1} | \\textbf{x_i}, \\textbf{w})) + (1 - y_i) \\log(1 - p(\\textbf{y=1} | \\textbf{x_i}, \\textbf{w})))$\n",
    "</font>\n",
    "\n",
    "\n",
    "Ref: https://github.com/jcjohnson/pytorch-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "\n",
    "<img width=300 src=\"https://github.com/gmum/nn2018/raw/master/lab/fig/2/gd.png\">\n",
    "<br>\n",
    "\n",
    "## Logistic regression\n",
    "\n",
    "<img width=600 src=\"https://github.com/gmum/nn2018/raw/master/lab/fig/2/graph_logreg.png\">\n",
    "<br>\n",
    "\n",
    "Gradient of the loss is given by:\n",
    "\n",
    "<br>\n",
    "<font size=5>\n",
    "$ \\frac{\\partial L(\\textbf{w})}{\\partial \\textbf{w}_i} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L(\\textbf{x_i, w})}{\\partial \\textbf{w}_i} = \\frac{1}{N} \\sum_{i=1}^{N} x_i (p_j - y_j)$\n",
    "</font>\n",
    "\n",
    ", where $p_i$ denotes prediction of the model on $i^{th}$ example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred: np.arrary, shape: (batch_size,)\n",
    "        Probabiities\n",
    "    y: np.array, shape: (batch_size,)\n",
    "        Correct classes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss: float\n",
    "        Cross entropy loss\n",
    "    \"\"\"\n",
    "    return - np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "def sigmoid(z): \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    z: np.arrary, shape: (batch_size,)\n",
    "        Logits\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred: np.arrary, shape: (batch_size,)\n",
    "        Probabiities\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def gradient(y_pred, y, x): \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred: np.arrary, shape: (batch_size,)\n",
    "        Probabiities\n",
    "    y: np.array, shape: (batch_size,)\n",
    "        Correct classes\n",
    "    x: np.array, shape: (batch_size, D)\n",
    "        Logits\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    grad: np.array, shape: (D, )\n",
    "        Array representing gradient of loss wrt to weight vector\n",
    "    \"\"\"\n",
    "    batch_size = len(y)\n",
    "    r = (y_pred - y).reshape(1, batch_size)\n",
    "    return (r.T * x).mean(axis=0)\n",
    "\n",
    "def forward(x, w):\n",
    "    return sigmoid(x.dot(w))\n",
    "\n",
    "def evaluate(w):\n",
    "    y_test_pred = forward(x_test, w)\n",
    "    return np.mean((y_test_pred>0.5) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Asserts to help you implement the functions.\n",
    "D = 785\n",
    "\n",
    "x = x_train[0:100]\n",
    "y = y_train[0:100]\n",
    "w = 0.01*np.ones_like(x_train[0, :])\n",
    "y_pred = forward(x, w)\n",
    "\n",
    "yours = forward(x, w)[0:4]\n",
    "correct = [0.95257767, 0.96536256, 0.75657633, 0.86287448]\n",
    "assert np.allclose(yours, correct, atol=0.01)\n",
    "\n",
    "yours = cross_entropy_loss(y_pred, y)\n",
    "correct = 2.087\n",
    "assert np.abs(yours - correct) < 0.01\n",
    "\n",
    "yours = gradient(y_pred, y, x)[0:4]\n",
    "correct = [7.53742754e-01, 0.00000000e+00, 0.00000000e+00, 3.50197809e-05]\n",
    "assert np.allclose(yours, correct, atol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_logistic_regression(lr=0.1, n_epochs=100, batch_size=100):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr: float\n",
    "        Learning rate used in SGD\n",
    "    n_epochs: int\n",
    "        Number of epochs to train\n",
    "    use_autograd: bool\n",
    "        If true will use PyTorch autograd\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w: np.array, shape: (D_in, D_out)\n",
    "        Found parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    n_epochs = 100\n",
    "    batch_size = 100\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    # 784 + bias -> 1 model\n",
    "    D = 784 + 1\n",
    "\n",
    "    # Define all Variables used in the computation\n",
    "    w = np.random.normal(size=(D, ))\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in tqdm.tqdm(range(n_epochs), total=n_epochs):    \n",
    "        for batch in range(len(x_train) // batch_size):\n",
    "            # Sample data\n",
    "            x_batch = x_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            y_batch = y_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            \n",
    "            y_pred = forward(x_batch, w)\n",
    "            loss = cross_entropy_loss(y_pred, y_batch)\n",
    "\n",
    "            if batch == 0:\n",
    "                loss_history.append(loss)\n",
    "\n",
    "            # Compute grad_w\n",
    "            grad_w = gradient(y_pred, y_batch, x_batch)\n",
    "\n",
    "            # Update weights using gradient descent\n",
    "            w -= learning_rate * grad_w\n",
    "\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "\n",
    "Each notebook will terminate with Tests section. This will automatically grade and assign points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 119.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "===========\n",
      "{'test1': 1}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAGwCAYAAADFUEBtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nohBsAICi/H93z9tQ3J898gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f678643ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = {}\n",
    "result['test1'] = int(evaluate(train_logistic_regression(n_epochs=100, lr=0.1)) > 0.8)\n",
    "print(\"Evaluation results:\\n===========\")\n",
    "print(result)\n",
    "json.dump(result, open(\"2a_logistic_regression.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nn2018]",
   "language": "python",
   "name": "conda-env-nn2018-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
