{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial logistic regression in numpy\n",
    "\n",
    "In this notebook we will refresh our knowledge of linear models. This notebook assumes familiarity with basic machine learning concepts. See \"Resources\" section for material if you are unfamiliar with some of the concepts.\n",
    "\n",
    "Your tasks will be:\n",
    "\n",
    "* Implement binary logistic regression using numpy\n",
    "* Implement SGD training loop \n",
    "\n",
    "Goal is to:\n",
    "\n",
    "* Get accustomed with the environment and grading\n",
    "* Refresh our numpy skills\n",
    "* Refresh our machine learning knowledge\n",
    "\n",
    "Next up:\n",
    "\n",
    "* Implementing multinomial logistic regression in PyTorch\n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "If content of this notebook is new, confusing or difficult - don't worry! Check out the following resources:\n",
    "* Logistic regression:  https://www.coursera.org/learn/neural-networks-deep-learning/lecture/yWaRd/logistic-regression-cost-function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get FashionMNIST (see 1b_FMNIST.ipynb for data exploration)\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Logistic regression needs 2D data\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# 0-1 normalization\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "# For simplicity - implicit bias\n",
    "x_train = np.concatenate([np.ones_like(x_train[:, 0:1]), x_train], axis=1)\n",
    "x_test = np.concatenate([np.ones_like(x_test[:, 0:1]), x_test], axis=1)\n",
    "\n",
    "# Use only first 1k examples. Just for notebook to run faster\n",
    "x_train, y_train = x_train[0:1000], y_train[0:1000]\n",
    "x_test, y_test = x_test[0:1000], y_test[0:1000]\n",
    "\n",
    "# Cast as binary\n",
    "y_train = (y_train == 0).astype(\"int\")\n",
    "y_test = (y_test == 0).astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy primer\n",
    "\n",
    "This section introduces needed concepts from Numpy to complete the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/gmum/nn2018/raw/master/lab/fig/2/python-comic-2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  [1 2 3 4]\n",
      "Pierwszy element:  1\n",
      "Drugi element!:  2\n",
      "a+1: [2 3 4 5]\n",
      "2*a: [2 4 6 8]\n",
      "b:  [1 2 3 4]\n",
      "Iloczyn skalarny <a,b>:  30\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "print \"a: \", a\n",
    "print \"Pierwszy element: \", a[0]\n",
    "print \"Drugi element!: \", a[1]\n",
    "print \"a+1:\", a + 1\n",
    "print \"2*a:\", 2*a\n",
    "b = np.array([1, 2, 3, 4])\n",
    "print \"b: \", b\n",
    "print \"Iloczyn skalarny <a,b>: \", np.inner(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "## Model\n",
    "\n",
    "\n",
    "Class prediction is given by:\n",
    "\n",
    "<br>\n",
    "<font size=4>\n",
    "$ p(\\textbf{y} | \\textbf{x}, \\textbf{w}) = \\sigma(\\langle\\textbf{w}, \\textbf{x}\\rangle)$\n",
    "</font>\n",
    "\n",
    ", where $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$.\n",
    "\n",
    "http://www.deeplearningbook.org/, Ch.1.\n",
    "\n",
    "<img width=600 src=\"https://github.com/gmum/nn2018/raw/master/lab/fig/1/fig4.png\">\n",
    "\n",
    "\n",
    "Loss is given by the cross-entropy loss:\n",
    "\n",
    "<br>\n",
    "<font size=4>\n",
    "$ L(\\textbf{w}) = \\sum_i y_i \\log(p(\\textbf{y_i} | \\textbf{x_i}, \\textbf{w}))$\n",
    "</font>\n",
    "\n",
    "\n",
    "Ref: https://github.com/jcjohnson/pytorch-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "\n",
    "<img width=300 src=\"https://github.com/gmum/nn2018/raw/master/lab/fig/2/gd.png\">\n",
    "<br>\n",
    "\n",
    "## Logistic regression\n",
    "\n",
    "<img width=600 src=\"https://github.com/gmum/nn2018/raw/master/lab/fig/2/graph_logreg.png\">\n",
    "<br>\n",
    "\n",
    "Gradient of the loss is given by:\n",
    "\n",
    "<br>\n",
    "<font size=5>\n",
    "$ \\frac{\\partial L(\\textbf{w})}{\\partial \\textbf{w}_i} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L(\\textbf{x_i, w})}{\\partial \\textbf{w}_i} = \\frac{1}{N} \\sum_{i=1}^{N} x_i (p_j - y_j)$\n",
    "</font>\n",
    "\n",
    ", where $p_i$ denotes prediction of the model on $i^{th}$ example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred: np.arrary, shape: (batch_size,)\n",
    "        Probabiities\n",
    "    y: np.array, shape: (batch_size,)\n",
    "        Correct classes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss: float\n",
    "        Cross entropy loss\n",
    "    \"\"\"\n",
    "    ??\n",
    "    \n",
    "def sigmoid(z): \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    z: np.arrary, shape: (batch_size,)\n",
    "        Logits\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred: np.arrary, shape: (batch_size,)\n",
    "        Probabiities\n",
    "    \"\"\"\n",
    "    ??\n",
    "\n",
    "\n",
    "def gradient(y_pred, y, x): \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred: np.arrary, shape: (batch_size,)\n",
    "        Probabiities\n",
    "    y: np.array, shape: (batch_size,)\n",
    "        Correct classes\n",
    "    x: np.array, shape: (batch_size, D)\n",
    "        Logits\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    grad: np.array, shape: (D, )\n",
    "        Array representing gradient of loss wrt to weight vector\n",
    "    \"\"\"\n",
    "    ??\n",
    "\n",
    "def forward(x, w):\n",
    "    return sigmoid(x.dot(w))\n",
    "\n",
    "def evaluate(w):\n",
    "    y_test_pred = forward(x_test, w)\n",
    "    return np.mean((y_test_pred>0.5) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Asserts to help you implement the functions.\n",
    "D = 785\n",
    "\n",
    "x = x_train[0:100]\n",
    "y = y_train[0:100]\n",
    "w = 0.01*np.ones_like(x_train[0, :])\n",
    "y_pred = forward(x, w)\n",
    "\n",
    "yours = forward(x, w)[0:4]\n",
    "correct = [0.95257767, 0.96536256, 0.75657633, 0.86287448]\n",
    "assert np.allclose(yours, correct, atol=0.01)\n",
    "\n",
    "yours = cross_entropy_loss(y_pred, y)\n",
    "correct = 2.087\n",
    "assert np.abs(yours - correct) < 0.01\n",
    "\n",
    "yours = gradient(y_pred, y, x)[0:4]\n",
    "correct = [7.53742754e-01, 0.00000000e+00, 0.00000000e+00, 3.50197809e-05]\n",
    "assert np.allclose(yours, correct, atol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_logistic_regression(lr=0.1, n_epochs=100, batch_size=100):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr: float\n",
    "        Learning rate used in SGD\n",
    "    n_epochs: int\n",
    "        Number of epochs to train\n",
    "    use_autograd: bool\n",
    "        If true will use PyTorch autograd\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w: np.array, shape: (D_in, D_out)\n",
    "        Found parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    n_epochs = 100\n",
    "    batch_size = 100\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    # 784 + bias -> 1 model\n",
    "    D = 784 + 1\n",
    "\n",
    "    # Define all Variables used in the computation\n",
    "    w = np.random.normal(size=(D, ))\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in tqdm.tqdm(range(n_epochs), total=n_epochs):    \n",
    "        for batch in range(len(x_train) // batch_size):\n",
    "            # Sample data\n",
    "            x_batch = x_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            y_batch = y_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            \n",
    "            y_pred = ??\n",
    "            loss = ??\n",
    "\n",
    "            if batch == 0:\n",
    "                loss_history.append(loss)\n",
    "\n",
    "            # Compute grad_w\n",
    "            grad_w = ??\n",
    "\n",
    "            # Update weights using gradient descent\n",
    "            w -= ??\n",
    "\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "\n",
    "Each notebook will terminate with Tests section. This will automatically grade and assign points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "===========\n",
      "{'test1': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcAAAAGwCAYAAAA+B7WyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1106910d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = {}\n",
    "result['test1'] = int(evaluate(train_logistic_regression(n_epochs=100, lr=0.1)) > 0.8)\n",
    "print(\"Evaluation results:\\n===========\")\n",
    "print(result)\n",
    "json.dump(result, open(\"2a_pytorch_result.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
