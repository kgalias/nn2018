{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Autoencoders (and unsupervised learning)\n",
    "\n",
    "Goal of the lab:\n",
    "* Understand basics of unsupervised learning\n",
    "* Understand autoencoders\n",
    "\n",
    "References:\n",
    "\n",
    "* http://www.deeplearningbook.org/contents/representation.html\n",
    "\n",
    "* http://curiousily.com/data-science/2017/02/02/what-to-do-when-data-is-missing-part-2.html\n",
    "\n",
    "* Content partially based on https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents-notebooks/404_autoencoder.ipynb (I highly recommend the whole repository)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whiteboard exercises\n",
    "\n",
    "(Any left out exercises from the previous labs)\n",
    "\n",
    "* (0.5) Explain importance of the bottleneck in vanilla autoencoders, i.e. why does input has to be strictly lower dimensional than the input. Describe a different way of achieving bottleneck than just reducing dimensionality (hint: lookup denoising autoencoders, contrastive autoencoders or variational autoencoders)\n",
    "\n",
    "* (0.5) Discuss role of pretraining in training deep models (see for instance http://www.deeplearningbook.org/contents/representation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "<img src=\"http://curiousily.com/assets/12.what_to_do_when_data_is_missing_part_ii_files/mushroom_encoder.png\", width=700>\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "\n",
    "Vanilla autoencoder minimizes a reconstruction error, usually given by the mean squared error between input and reconstructed input.\n",
    "\n",
    "</font></p>\n",
    "\n",
    "<font size=4>\n",
    "$$L(x, dec(enc(x))) = ||x - dec(enc(x))||^2$$\n",
    "</font>\n",
    "\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "Encoder and decoder are arbitrary neural networks. Usually decoder in vanilla autoencoders follow the same transformations as encoder layers, but in a reversed order. To prevent finding trivial solutions it is necessary to use a smaller dimensionality of the encoer output.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "%matplotlib inline\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)    # reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "EPOCH = 10\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.005         # learning rate\n",
    "DOWNLOAD_MNIST = False\n",
    "N_TEST_IMG = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mnist digits dataset\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist/',\n",
    "    train=True,                                     # this is training data\n",
    "    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n",
    "                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n",
    "    download=DOWNLOAD_MNIST,                        # download it if you don't have it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Vanilla autoencoder\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Finish implementation of autoencoder\n",
    "2. Save reconstructions from the final epoch to 11a_1.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, D=3):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(12, D),   # compress to 3 features which can be visualized in plt\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            ??, # Layers in the decoder in autoencoders usually follow a reverse order\n",
    "            ??,\n",
    "            nn.Linear(12, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 28*28),\n",
    "            nn.Sigmoid(),       # compress to a range (0, 1)\n",
    "        )\n",
    "        \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ?? # Encode and decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder()\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR)\n",
    "loss_func = ?? # Use MSE loss function\n",
    "\n",
    "# original data (first row) for viewing\n",
    "view_data = Variable(train_data.train_data[:N_TEST_IMG].view(-1, 28*28).type(torch.FloatTensor)/255.)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        b_x = Variable(x.view(-1, 28*28))   # batch x, shape (batch, 28*28)\n",
    "        b_y = Variable(x.view(-1, 28*28))   # batch y, shape (batch, 28*28)\n",
    "        b_label = Variable(y)               # batch label\n",
    "\n",
    "        encoded, decoded = ??\n",
    "\n",
    "        loss = ?? # See equations for autoencoder\n",
    "        optimizer.zero_grad()               # clear gradients for this training step\n",
    "        loss.backward()                     # backpropagation, compute gradients\n",
    "        optimizer.step()                    # apply gradients\n",
    "\n",
    "        if step % 500 == 0 and epoch in [0, 5, EPOCH-1]:\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data[0])\n",
    "\n",
    "            # plotting decoded image (second row)\n",
    "            _, decoded_data = autoencoder(view_data)\n",
    "            \n",
    "            # initialize figure\n",
    "            f, a = plt.subplots(2, N_TEST_IMG, figsize=(5, 2))\n",
    "            \n",
    "            for i in range(N_TEST_IMG):\n",
    "                a[0][i].imshow(np.reshape(view_data.data.numpy()[i], (28, 28)), cmap='gray'); a[0][i].set_xticks(()); a[0][i].set_yticks(())\n",
    "    \n",
    "            for i in range(N_TEST_IMG):\n",
    "                a[1][i].clear()\n",
    "                a[1][i].imshow(np.reshape(decoded_data.data.numpy()[i], (28, 28)), cmap='gray')\n",
    "                a[1][i].set_xticks(()); a[1][i].set_yticks(())\n",
    "            plt.show(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: sampling and interpolation using an autoencoder\n",
    "\n",
    "Task:\n",
    "1. Fill up missing parts in sampling code and generate 5 examples. Save results to 11a_2a.png\n",
    "2. Fill up missing parts in interpolation code. Provide an interpolation between for 3 pairs of training examples. Save figures to 11a_2b_%id.png\n",
    "3. Change architecture of the autoencoder to 128 -> 128 -> 128, with a Tanh nonlinearity in the middle, i.e. enc(x) = Tanh(Wx+b) and dec(z)=Sigmoid(Uz). Train this autoencoder. Compare sampling and interpolation results for this model and describe what you observed in 11a_2c.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Theoretically speaking, we cannot sample from autoencoder. But we can! Let's generate new digits.\n",
    "\n",
    "The heuristic algorithm for sampling from vanilla autoencoder is:\n",
    "1. Compute mean and std of the dataset\n",
    "2. Sample $z$ from a Gaussian distribution using the estimated mean and std\n",
    "3. Decode the sampled $z$ \n",
    "\n",
    "Expected result (of course you might sample different digits) <img src=\"fig/11/expected_11a_2a.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "for step, (x, y) in enumerate(train_loader):\n",
    "    if len(samples) > 100:\n",
    "        break\n",
    "    sample = x.view(-1, 28*28)    \n",
    "    enc = autoencoder.encode(Variable(sample))\n",
    "    samples.append(enc.data.numpy())\n",
    "sampled_z = np.concatenate(samples, axis=0)\n",
    "mean_Z, std_Z = ??, ?? # Compute mean and std of the empirical distribution in the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, a = plt.subplots(1, N_TEST_IMG, figsize=(5, 2))\n",
    "            \n",
    "for i in range(N_TEST_IMG):\n",
    "    z_sampled = ??\n",
    "    x_decoded = ??\n",
    "    a[i].imshow(np.reshape(x_decoded.data.numpy(), (28, 28)), cmap='gray'); \n",
    "    a[i].set_xticks(()); \n",
    "    a[i].set_yticks(())\n",
    "\n",
    "plt.savefig(\"11a_2a.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation\n",
    "\n",
    "Interpolation:\n",
    "1. Encode samples $z_a=enc(x_a)$ and $z_b=enc(x_b)$\n",
    "2. Let $z(\\alpha)=\\alpha z_a + (1-\\alpha) z_b$\n",
    "3. Let $x(\\alpha) = dec(z(\\alpha))$\n",
    "\n",
    "Example result <img src=\"fig/11/expected_11a_2b.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "for step, (x, y) in enumerate(train_loader):\n",
    "    if step > 3:\n",
    "        break\n",
    "        \n",
    "    samples.append(1)\n",
    "    x_a = x[0:1, :]\n",
    "    x_b = x[1:2, :]\n",
    "    \n",
    "    z_a = ??\n",
    "    z_b = ??\n",
    "    \n",
    "    f, a = plt.subplots(1, 10, figsize=(10, 3))\n",
    "    \n",
    "    for i, alpha in enumerate(np.linspace(0, 1, 10)):\n",
    "        z_int = ??\n",
    "        a[i].imshow(np.reshape(x_int.data.numpy(), (28, 28)), cmap='gray'); \n",
    "        a[i].set_xticks(()); \n",
    "        a[i].set_yticks(())   \n",
    "    \n",
    "    plt.savefig(\"11a_2b_{}.png\".format(step))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
