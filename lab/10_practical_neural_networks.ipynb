{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuts and bolts of \"Neural Networking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will try to fit best NN for solving Fashion MNIST (subset to 1k examples) following recommendation from Andrew Ng lecture on how to train NNs in practice.\n",
    "\n",
    "Goal is to:\n",
    "\n",
    "* Get familiar with various basic building blocks\n",
    "* Understand \"nuts and bolts\" of training DNNs\n",
    "    * Understand why we need train, valid and test split\n",
    "    * Understand how to reduce bias, variance\n",
    "    * Understand notions of human error\n",
    "    * Get to know the heuristical DL workflow (overfit -> regularize -> revise priors)\n",
    "\n",
    "When you are done, please compile a simple pdf report (e.g. you can copy paste figures into a google doc, and save as a pdf) and put it into Dropbox folder. \n",
    "\n",
    "Do a bunch of experiments, explain why something helps.\n",
    "\n",
    "Refs:\n",
    "\n",
    "* Nuts and bolts of applying Deep Learning: https://www.youtube.com/watch?v=F1ka6a13S9I , summary http://jaejunyoo.blogspot.com/2017/03/nips-2016-tutorial-summary-nuts-and-bolts-of-building-AI-AndrewNg.html\n",
    "* Introduction to Convolutional networks: http://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andrew Ng's \"Nuts and bolts\"\n",
    "\n",
    "Nuts and bolts of applying Deep Learning: https://www.youtube.com/watch?v=F1ka6a13S9I\n",
    "\n",
    "Some of them might sound very weird, but use as much as you can in this notebook. We will be coming back to them.\n",
    "\n",
    "Note: This is very likely to be an exam question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The general workflow\n",
    "\n",
    "Let's define overfitting for our purposes as achieving ~100% training accuracy (which is almost never possible to achieve on validation set).\n",
    "\n",
    "<img width=400 src=\"https://3.bp.blogspot.com/-duzBNDYdDGA/WFNtNi0DcNI/AAAAAAAAPSc/AHuvDXl6EhAgweD6IxGAbqOBK5qM_W05QCLcB/s1600/nuts-and-bolts-checklist.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias vs variance\n",
    "\n",
    "\"It takes surprisingly long time to grok bias and variance deeply, but people that understand bias and variance deeply are often able to drive very rapid progress.\" --Andrew Ng \n",
    "\n",
    "TODO: Explanation\n",
    "\n",
    "<img width=500 src=\"http://1.bp.blogspot.com/-IKBOtqKxf6M/WL4VFKZsI7I/AAAAAAAABYY/vOuV7QmBJSU6ca5vo3I8tzULMwtx5xInACK4B/s1600/andrewNg_8.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use DNNs only when you have a lot of data\n",
    "\n",
    "Always use more data\n",
    "\n",
    "<img width=500 src=\"https://github.com/gmum/nn2018/raw/master/lab/fig/7/perf.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of things we can tune\n",
    "\n",
    "* Add/remove blocks:\n",
    "    - Batch Normalization\n",
    "    - Dropout\n",
    "    - Convolution\n",
    "    - Pooling\n",
    "    - Dense\n",
    "    - Activation\n",
    "* Tune regularization\n",
    "    - Dropout\n",
    "    - L2\n",
    "* Alter parameters of blocks\n",
    "    - Number of units\n",
    "    - Nonlinearity type\n",
    "* Change optimization hyperparameters\n",
    "    - Learning rate\n",
    "    - Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Get FashionMNIST (see 1b_FMNIST.ipynb for data exploration)\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Logistic regression needs 2D data\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# 0-1 normalization\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "# Convert to Torch Tensor. Just to avoid boilerplate code later\n",
    "x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "# Use only first 1k examples. Just for notebook to run faster\n",
    "x_valid, y_valid = x_train[1000:2000], y_train[1000:2000]\n",
    "x_train, y_train = x_train[0:1000], y_train[0:1000]\n",
    "x_test, y_test = x_test[0:1000], y_test[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting point\n",
    "\n",
    "This section gives basic model. Please adapt yourself training loop from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_mlp(input_dim, output_dim):\n",
    "    model = torch.nn.Sequential()\n",
    "    model.add_module(\"linear_1\", torch.nn.Linear(input_dim, 512, bias=False))\n",
    "    model.add_module(\"nonlinearity_1\", torch.nn.Sigmoid())\n",
    "    model.add_module(\"linear_2\", torch.nn.Linear(512, output_dim, bias=False))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Our goal is to go through different types of blocks without very in-depth understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple tuning routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - overfit using large MLP (bias from N&B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - regularize (variance from N&B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which one (alone) was most effective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - better adaptation (train-test mismatch from N&B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune CNN architecture\n",
    "\n",
    "Compare a good CNN (tune its hyperparameters on valid) to a good MLP (tune its hyperparameters on valid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune architecture: CNN vs MLP\n",
    "\n",
    "We will have separate lab on convolutions. A crash course on CNNs:\n",
    "\n",
    "<img width=300 src=http://cs231n.github.io/assets/nn1/neural_net2.jpeg>\n",
    "\n",
    "<img width=400 src=http://cs231n.github.io/assets/cnn/stride.jpeg>\n",
    "\n",
    "CNN hyperparameters:\n",
    "\n",
    "* Number of filters\n",
    "* Filter size\n",
    "* Stride (less important usually)\n",
    "\n",
    "Ref: \n",
    "* Images from http://cs231n.github.io/convolutional-networks/\n",
    "* How to create CNNs in PyTorch https://github.com/vinhkhuc/PyTorch-Mini-Tutorials/blob/master/5_convolutional_net.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra tuning - LR regularization effect\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
