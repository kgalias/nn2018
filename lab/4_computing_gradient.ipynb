{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing gradient\n",
    "\n",
    "Training DNNs requires computing gradient of the loss with respect to the parameters, and this is topic of this notebook.\n",
    "\n",
    "Your task will be to:\n",
    "\n",
    "* Implement Evolution Search training of DNNs that does not require backpropagation\n",
    "* Implement forward and backward functions for Linear and ReQU modules\n",
    "\n",
    "Goal is to:\n",
    "\n",
    "* Introduce ingredients necessary to train NN:\n",
    "    * model\n",
    "    * loss\n",
    "    * optimizer\n",
    "* Introduce different ways of computing or estimating $\\frac{\\partial{L(w)}}{{\\partial w}}$\n",
    "* Get understanding of Evolution Search and Backpropagation algorithms\n",
    "\n",
    "Exam:\n",
    "\n",
    "* For exam you are expected to understand how backpropagation works. Questions might refer to implementation details in PyTorch. See also Resources section for a good video about backpropgation, and how it is implemented in PyTorch.\n",
    "\n",
    "What's (probably) next:\n",
    "\n",
    "* Go through each component of the training loop:\n",
    "    * (Lab 5) Neural Networks practical, part 1: architecture, understanding what is learned\n",
    "    * (Lab 6, ?) Neural Networks practical part 2: setting up data and loss\n",
    "    * (Lab 7, ?) Neural Networks practical part 3: optimization\n",
    "\n",
    "Resources:\n",
    "\n",
    "* Backprop with focus on PyTorch: https://www.youtube.com/watch?v=ma2KXWblllc (see also other lectures from this series)\n",
    "\n",
    "* Lecture on backpropagation https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/ (Lecture 8)\n",
    "\n",
    "* Evolution Search https://eng.uber.com/deep-neuroevolution/, https://arxiv.org/abs/1712.06564\n",
    "\n",
    "* Matrix derivatives http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD requires $\\frac{\\partial L (w) }{\\partial w}$!\n",
    "\n",
    "<p>\n",
    "<font size=5>\n",
    "$$\\frac{\\partial L (w) }{\\partial w} = \\frac{1}{K} \\sum_{i=1}^{K} \\frac{\\partial L (x_i, w) }{\\partial w}$$\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<img width=600 src=\"https://github.com/gmum/nn2018/raw/master/lab/fig/3/sgd.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Get FashionMNIST (see 1b_FMNIST.ipynb for data exploration)\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Logistic regression needs 2D data\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# 0-1 normalization\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "# Convert to Torch Tensor. Just to avoid boilerplate code later\n",
    "x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "# Use only first 1k examples. Just for notebook to run faster\n",
    "x_train, y_train = x_train[0:1000], y_train[0:1000]\n",
    "x_test, y_test = x_test[0:1000], y_test[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Some helper functions\n",
    "\n",
    "def get_model_weights(model):\n",
    "    params = model.state_dict()\n",
    "    p_order = [p[0] for p in sorted(model.named_parameters())]\n",
    "    return np.concatenate([params[w].cpu().numpy().reshape(-1, ) for w in p_order])\n",
    "    \n",
    "def set_model_weights(model, w):\n",
    "    params = model.state_dict()  \n",
    "    p_order = [p[0] for p in sorted(model.named_parameters())]\n",
    "    id = 0\n",
    "    for p in p_order:\n",
    "        shape = params[p].shape\n",
    "        D = np.prod(shape)\n",
    "        params[p].copy_(torch.from_numpy(w[id:id + D].reshape(shape)))\n",
    "        id += D\n",
    "        \n",
    "def L(w, loss, model, batch_size=10, x_tr=x_train, y_tr=y_train):\n",
    "    set_model_weights(model, w)\n",
    "    cost = 0.\n",
    "    n_examples, n_features = x_tr.size()\n",
    "    num_batches = n_examples // batch_size\n",
    "    for k in range(num_batches): \n",
    "        start, end = k * batch_size, (k + 1) * batch_size\n",
    "        x = Variable(x_tr[start:end], requires_grad=False)\n",
    "        y = Variable(y_tr[start:end], requires_grad=False)\n",
    "        fx = model.forward(x)\n",
    "        cost += loss.forward(fx, y)\n",
    "    return cost.data.numpy()[0] / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop in PyTorch\n",
    "\n",
    "In this section we walk through implementation of a traditional training loop  in PyTorch.\n",
    "\n",
    "Necessary ingredients to train a neural network are:\n",
    "    * model\n",
    "    * loss\n",
    "    * optimizer\n",
    "    \n",
    "<img width=400 src=\"https://github.com/gmum/nn2018/raw/master/lab/fig/4/smoothie.png\">\n",
    "\n",
    "Ref:\n",
    "\n",
    "https://github.com/vinhkhuc/PyTorch-Mini-Tutorials/blob/master/3_neural_net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_logreg(input_dim, output_dim, weight_module=torch.nn.Linear):\n",
    "    model = torch.nn.Sequential()\n",
    "    model.add_module(\"linear_2\", weight_module(input_dim, output_dim, bias=False))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(input_dim, output_dim, nonlinearity=torch.nn.functional.sigmoid, weight_module=torch.nn.Linear):\n",
    "    class Nonlinearity(torch.nn.Module):\n",
    "        def forward(self, input):\n",
    "            return nonlinearity(input)\n",
    "    \n",
    "    model = torch.nn.Sequential()\n",
    "    model.add_module(\"linear_1\", weight_module(input_dim, 512, bias=False))\n",
    "    model.add_module(\"nonlinearity_1\", Nonlinearity())\n",
    "    model.add_module(\"linear_2\", weight_module(512, output_dim, bias=False))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_step(model, loss, x_val, y_val, lr=0.1, momentum=0.9):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "    x = Variable(x_val, requires_grad=False)\n",
    "    y = Variable(y_val, requires_grad=False)\n",
    "\n",
    "    # Reset gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    fx = model.forward(x)\n",
    "    output = loss.forward(fx, y)\n",
    "\n",
    "    # Backward\n",
    "    output.backward(retain_graph=True)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return output.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, x_val):\n",
    "    x = Variable(x_val, requires_grad=False)\n",
    "    output = model.forward(x)\n",
    "    return output.data.numpy().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, loss, step_fnc):\n",
    "    torch.manual_seed(42)\n",
    "    n_examples, n_features = x_train.size()\n",
    "    n_classes = 10\n",
    "    batch_size = 100\n",
    "    history = []\n",
    "    for i in tqdm.tqdm(range(100), total=100):\n",
    "        cost = 0.\n",
    "        num_batches = n_examples // batch_size\n",
    "        for k in range(num_batches):\n",
    "            start, end = k * batch_size, (k + 1) * batch_size\n",
    "            cost += step_fnc(model, loss, x_train[start:end], y_train[start:end])\n",
    "        predY = predict(model, x_test)\n",
    "        print((\"Epoch %d, cost = %f, acc = %.2f%%\"\n",
    "              % (i + 1, cost / num_batches, 100. * np.mean(predY == y_test.numpy()))))\n",
    "        \n",
    "        history.append(cost)\n",
    "        \n",
    "    return np.mean(predY == y_test.numpy()), history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:30,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, cost = 2.365059, acc = 9.50%\n",
      "Epoch 2, cost = 2.273749, acc = 9.80%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/100 [00:00<00:24,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3, cost = 2.156826, acc = 16.80%\n",
      "Epoch 4, cost = 2.028474, acc = 30.40%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 4/100 [00:00<00:19,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5, cost = 1.894913, acc = 37.00%\n",
      "Epoch 6, cost = 1.765987, acc = 44.70%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 6/100 [00:00<00:15,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7, cost = 1.649051, acc = 50.00%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 7/100 [00:00<00:13,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8, cost = 1.546784, acc = 54.30%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 8/100 [00:01<00:12,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9, cost = 1.458591, acc = 57.30%\n",
      "Epoch 10, cost = 1.382546, acc = 58.90%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 10/100 [00:01<00:11,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11, cost = 1.316553, acc = 61.00%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 11/100 [00:01<00:10,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12, cost = 1.258788, acc = 62.00%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 12/100 [00:01<00:10,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13, cost = 1.207788, acc = 63.50%\n",
      "Epoch 14, cost = 1.162407, acc = 64.50%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 14/100 [00:01<00:09,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15, cost = 1.121747, acc = 66.40%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 15/100 [00:01<00:11,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16, cost = 1.085095, acc = 66.80%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 16/100 [00:01<00:11,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17, cost = 1.051879, acc = 67.60%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 17/100 [00:02<00:11,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18, cost = 1.021635, acc = 68.00%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 18/100 [00:02<00:10,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19, cost = 0.993979, acc = 68.80%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 19/100 [00:02<00:09,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20, cost = 0.968594, acc = 69.10%\n",
      "Epoch 21, cost = 0.945212, acc = 69.10%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 21/100 [00:02<00:08,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22, cost = 0.923607, acc = 69.20%\n",
      "Epoch 23, cost = 0.903586, acc = 69.60%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 23/100 [00:02<00:07, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24, cost = 0.884983, acc = 70.40%\n",
      "Epoch 25, cost = 0.867653, acc = 70.50%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 25/100 [00:02<00:07, 10.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26, cost = 0.851469, acc = 70.70%\n",
      "Epoch 27, cost = 0.836320, acc = 70.70%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 27/100 [00:02<00:07, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28, cost = 0.822107, acc = 70.90%\n",
      "Epoch 29, cost = 0.808743, acc = 71.40%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 29/100 [00:03<00:06, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30, cost = 0.796150, acc = 71.50%\n",
      "Epoch 31, cost = 0.784256, acc = 71.90%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 31/100 [00:03<00:06, 10.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32, cost = 0.773000, acc = 72.00%\n",
      "Epoch 33, cost = 0.762325, acc = 72.20%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 33/100 [00:03<00:06, 10.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34, cost = 0.752181, acc = 72.10%\n",
      "Epoch 35, cost = 0.742521, acc = 72.50%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 35/100 [00:03<00:06, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36, cost = 0.733305, acc = 72.70%\n",
      "Epoch 37, cost = 0.724496, acc = 73.30%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 37/100 [00:03<00:05, 10.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38, cost = 0.716060, acc = 73.70%\n",
      "Epoch 39, cost = 0.707967, acc = 73.90%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 39/100 [00:04<00:05, 10.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40, cost = 0.700191, acc = 73.90%\n",
      "Epoch 41, cost = 0.692706, acc = 74.00%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████      | 41/100 [00:04<00:05, 10.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42, cost = 0.685491, acc = 74.00%\n",
      "Epoch 43, cost = 0.678524, acc = 73.90%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 43/100 [00:04<00:05, 10.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44, cost = 0.671789, acc = 73.90%\n",
      "Epoch 45, cost = 0.665269, acc = 74.10%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 45/100 [00:04<00:05, 10.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46, cost = 0.658948, acc = 74.20%\n",
      "Epoch 47, cost = 0.652814, acc = 74.20%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 47/100 [00:04<00:04, 10.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48, cost = 0.646853, acc = 74.30%\n",
      "Epoch 49, cost = 0.641055, acc = 74.30%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████▉     | 49/100 [00:05<00:05, 10.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50, cost = 0.635409, acc = 74.20%\n",
      "Epoch 51, cost = 0.629906, acc = 74.40%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████     | 51/100 [00:05<00:05,  9.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52, cost = 0.624538, acc = 74.40%\n",
      "Epoch 53, cost = 0.619297, acc = 74.50%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 53/100 [00:05<00:04, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54, cost = 0.614176, acc = 74.40%\n",
      "Epoch 55, cost = 0.609169, acc = 74.40%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 55/100 [00:05<00:04, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56, cost = 0.604269, acc = 74.60%\n",
      "Epoch 57, cost = 0.599472, acc = 74.60%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 57/100 [00:05<00:04, 10.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58, cost = 0.594772, acc = 74.70%\n",
      "Epoch 59, cost = 0.590164, acc = 75.20%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▉    | 59/100 [00:05<00:04, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60, cost = 0.585645, acc = 75.30%\n",
      "Epoch 61, cost = 0.581211, acc = 75.50%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████    | 61/100 [00:06<00:03, 10.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62, cost = 0.576859, acc = 75.70%\n",
      "Epoch 63, cost = 0.572584, acc = 75.90%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 63/100 [00:06<00:03, 10.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64, cost = 0.568384, acc = 76.20%\n",
      "Epoch 65, cost = 0.564256, acc = 76.10%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 65/100 [00:06<00:03, 10.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66, cost = 0.560198, acc = 76.40%\n",
      "Epoch 67, cost = 0.556207, acc = 76.50%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 67/100 [00:06<00:03, 10.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68, cost = 0.552281, acc = 76.50%\n",
      "Epoch 69, cost = 0.548418, acc = 76.50%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▉   | 69/100 [00:06<00:02, 10.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70, cost = 0.544616, acc = 76.50%\n",
      "Epoch 71, cost = 0.540873, acc = 76.60%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 71/100 [00:07<00:02, 10.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72, cost = 0.537187, acc = 76.60%\n",
      "Epoch 73, cost = 0.533557, acc = 76.60%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 73/100 [00:07<00:02, 10.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 74, cost = 0.529981, acc = 76.50%\n",
      "Epoch 75, cost = 0.526458, acc = 76.60%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 75/100 [00:07<00:02, 10.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76, cost = 0.522986, acc = 76.60%\n",
      "Epoch 77, cost = 0.519565, acc = 76.70%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 77/100 [00:07<00:02, 10.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78, cost = 0.516193, acc = 76.70%\n",
      "Epoch 79, cost = 0.512868, acc = 76.80%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 79/100 [00:07<00:01, 11.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80, cost = 0.509590, acc = 76.90%\n",
      "Epoch 81, cost = 0.506358, acc = 76.70%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████  | 81/100 [00:07<00:01, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82, cost = 0.503171, acc = 77.00%\n",
      "Epoch 83, cost = 0.500028, acc = 77.00%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 83/100 [00:08<00:01, 11.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84, cost = 0.496927, acc = 77.20%\n",
      "Epoch 85, cost = 0.493868, acc = 77.20%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 85/100 [00:08<00:01, 10.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86, cost = 0.490851, acc = 77.20%\n",
      "Epoch 87, cost = 0.487873, acc = 77.30%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 87/100 [00:08<00:01, 11.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88, cost = 0.484935, acc = 77.30%\n",
      "Epoch 89, cost = 0.482036, acc = 77.30%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 89/100 [00:08<00:00, 11.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90, cost = 0.479175, acc = 77.50%\n",
      "Epoch 91, cost = 0.476351, acc = 77.70%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 91/100 [00:08<00:00, 11.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92, cost = 0.473563, acc = 77.70%\n",
      "Epoch 93, cost = 0.470811, acc = 77.80%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 93/100 [00:09<00:00, 11.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 94, cost = 0.468094, acc = 77.80%\n",
      "Epoch 95, cost = 0.465412, acc = 77.80%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 95/100 [00:09<00:00, 12.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96, cost = 0.462764, acc = 77.80%\n",
      "Epoch 97, cost = 0.460148, acc = 77.80%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 97/100 [00:09<00:00, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98, cost = 0.457565, acc = 77.90%\n",
      "Epoch 99, cost = 0.455014, acc = 77.90%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████▉| 99/100 [00:09<00:00, 12.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100, cost = 0.452495, acc = 77.90%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.779,\n",
       " [23.65058732032776,\n",
       "  22.73749351501465,\n",
       "  21.568257093429565,\n",
       "  20.284743905067444,\n",
       "  18.94912815093994,\n",
       "  17.65986692905426,\n",
       "  16.490514278411865,\n",
       "  15.467840433120728,\n",
       "  14.585906982421875,\n",
       "  13.825463056564331,\n",
       "  13.165529131889343,\n",
       "  12.587876796722412,\n",
       "  12.07787811756134,\n",
       "  11.624071955680847,\n",
       "  11.217470169067383,\n",
       "  10.8509521484375,\n",
       "  10.51879495382309,\n",
       "  10.216351449489594,\n",
       "  9.93979287147522,\n",
       "  9.685938358306885,\n",
       "  9.452118039131165,\n",
       "  9.236069560050964,\n",
       "  9.035862863063812,\n",
       "  8.849831700325012,\n",
       "  8.676527976989746,\n",
       "  8.51468962430954,\n",
       "  8.363200962543488,\n",
       "  8.221074998378754,\n",
       "  8.087433516979218,\n",
       "  7.961495280265808,\n",
       "  7.842560291290283,\n",
       "  7.729999780654907,\n",
       "  7.623251020908356,\n",
       "  7.521805226802826,\n",
       "  7.4252079129219055,\n",
       "  7.33304899930954,\n",
       "  7.2449570298194885,\n",
       "  7.160598874092102,\n",
       "  7.079673588275909,\n",
       "  7.0019097328186035,\n",
       "  6.92706161737442,\n",
       "  6.854906857013702,\n",
       "  6.785244703292847,\n",
       "  6.71789425611496,\n",
       "  6.652690172195435,\n",
       "  6.589481890201569,\n",
       "  6.528135597705841,\n",
       "  6.46852719783783,\n",
       "  6.410545349121094,\n",
       "  6.354087769985199,\n",
       "  6.299060761928558,\n",
       "  6.245381653308868,\n",
       "  6.192972540855408,\n",
       "  6.141763985157013,\n",
       "  6.091689467430115,\n",
       "  6.042692005634308,\n",
       "  5.994717061519623,\n",
       "  5.9477152824401855,\n",
       "  5.901641666889191,\n",
       "  5.856453835964203,\n",
       "  5.812113642692566,\n",
       "  5.768586039543152,\n",
       "  5.725838303565979,\n",
       "  5.683840304613113,\n",
       "  5.642564117908478,\n",
       "  5.601982951164246,\n",
       "  5.5620728731155396,\n",
       "  5.522813051939011,\n",
       "  5.484181046485901,\n",
       "  5.446158558130264,\n",
       "  5.408726155757904,\n",
       "  5.371867686510086,\n",
       "  5.335566967725754,\n",
       "  5.2998082637786865,\n",
       "  5.264578104019165,\n",
       "  5.229862570762634,\n",
       "  5.195649951696396,\n",
       "  5.161926090717316,\n",
       "  5.1286813616752625,\n",
       "  5.095904469490051,\n",
       "  5.063584417104721,\n",
       "  5.0317109525203705,\n",
       "  5.000275373458862,\n",
       "  4.969269245862961,\n",
       "  4.938681334257126,\n",
       "  4.90850505232811,\n",
       "  4.878730893135071,\n",
       "  4.849351525306702,\n",
       "  4.820359319448471,\n",
       "  4.791746646165848,\n",
       "  4.7635054886341095,\n",
       "  4.735628813505173,\n",
       "  4.708110719919205,\n",
       "  4.680944114923477,\n",
       "  4.654120564460754,\n",
       "  4.6276355385780334,\n",
       "  4.601481884717941,\n",
       "  4.575653433799744,\n",
       "  4.550144195556641,\n",
       "  4.524947464466095])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(784, 10)\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "train(step_fnc=sgd_step, loss=loss, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating $\\frac{\\partial L (w) }{\\partial w}$\n",
    "\n",
    "In this section we will implement different gradient computation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Difference\n",
    "<p>\n",
    "<font size=4>\n",
    "Assuming we know direction we want to optimize in, we can approximate derivative simply by calculating\n",
    "\n",
    "$$\\langle \\frac{\\partial L (w) }{\\partial w}, \\vec{v} \\rangle \\approx \\frac{L(w) - L(w + v)}{|\\vec{v}|}$$\n",
    "\n",
    "\n",
    ", where $L(w)$ denotes esimation of loss (e.g. over whole training set, over $5\\%$ etc.). \n",
    "\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "As we will see later PyTorch uses Finite Difference method in gradient checks of automatic diffentiation modules.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution Search\n",
    "\n",
    "<p>\n",
    "Intuitively, Evolution Search (ES) approximates gradient by computing multiple times finite difference. Importantly it works for non-differentiable costs as well.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "$$ \\frac{\\partial L (w) }{\\partial w} \\approx \\frac{1}{N \\sigma} \\sum \\vec{v} ( L(w + \\vec{v}) - L(w) )$$\n",
    "\n",
    "\n",
    ", where $\\epsilon$ is a normally distributed vector sampled from gauss of standard deviation $\\sigma$, and $L(w)$ denotes esimation of loss (e.g. over whole training set, over $5\\%$ etc.). \n",
    "\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "Note: while it is recommended to not use minibatching in ES, for simplicity we will.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Evolution Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-31-1f1613c57dc5>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-1f1613c57dc5>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    v = ?? # Hint: use rng\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def ES_grad(model, loss, x_tr, y_tr, sigma=0.001, N=100):\n",
    "    \"\"\"\n",
    "    Estimate dL/dw using Evolution Search.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    model: torch.nn.Model\n",
    "        Model to take optimization step on\n",
    "    loss: torch.nn.Module\n",
    "        Loss function to optimize\n",
    "    x_tr: torch.Tensor, (n_examples, n_features)\n",
    "        Batch to compute gradient over\n",
    "    y_tr: torch.Tensor, (n_examples, )\n",
    "        Batch to compute gradient over\n",
    "    sigma: float\n",
    "    \n",
    "    N: int\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    g: np.vector, size: (D, )\n",
    "        Gradient of loss with respect to model's weights\n",
    "    \"\"\"\n",
    "    grad = 0\n",
    "    init_w = get_model_weights(model)\n",
    "    rng = np.random.RandomState()\n",
    "    base_loss = L(w=init_w, model=model, loss=loss, x_tr=x_tr, y_tr=y_tr)\n",
    "    for _ in tqdm.tqdm(range(N), total=N):\n",
    "        v = ?? # Hint: use rng\n",
    "        reward = ?? # Hint: use L\n",
    "        grad += ?? # Hint: see equation\n",
    "    return ?? # Hint: normalize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If code is slow, you can use build_logreg !\n",
    "# model = build_logreg(784, 10) \n",
    "model = build_model(784, 10)\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "init_w = get_model_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "down = 0\n",
    "for _ in range(10):\n",
    "    loss_before = L(w=init_w, model=model, loss=loss)\n",
    "    set_model_weights(model=model, w=init_w)\n",
    "    v = ES_grad(model, loss, x_tr=x_train, y_tr=y_train, sigma=0.002, N=50)\n",
    "    loss_after = L(w=init_w - 0.1*v, model=model, loss=loss)\n",
    "    if loss_before > loss_after:\n",
    "        down += 1\n",
    "        print(\"Went down!\")\n",
    "print(\"{}/{}\".format(down, 10))\n",
    "# This code is stochastic, so of course sometimes this assert can fail. \n",
    "assert down > 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement own module\n",
    "\n",
    "PyTorch enable defining own Modules. Here we will implement Linear and ReQu modules ourselves, to understand better how backpropagation works. \n",
    "\n",
    "Below you can find an example Module implementing the classial ReLU nonlinearity.\n",
    "\n",
    "Ref: https://github.com/jcjohnson/pytorch-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "  \"\"\"\n",
    "  We can implement our own custom autograd Functions by subclassing\n",
    "  torch.autograd.Function and implementing the forward and backward passes\n",
    "  which operate on Tensors.\n",
    "  \"\"\"\n",
    "  def forward(self, input):\n",
    "    \"\"\"\n",
    "    In the forward pass we receive a Tensor containing the input and return a\n",
    "    Tensor containing the output. You can cache arbitrary Tensors for use in the\n",
    "    backward pass using the save_for_backward method.\n",
    "    \"\"\"\n",
    "    self.save_for_backward(input)\n",
    "    return input.clamp(min=0)\n",
    "\n",
    "  def backward(self, grad_output):\n",
    "    \"\"\"\n",
    "    In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "    with respect to the output, and we need to compute the gradient of the loss\n",
    "    with respect to the input.\n",
    "    \"\"\"\n",
    "    input, = self.saved_tensors\n",
    "    grad_input = grad_output.clone()\n",
    "    print(grad_input)\n",
    "    grad_input[input < 0] = 0\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  0  0  0\n",
      " 0  0  0  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 1  0  0  0\n",
      " 0  0  0  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  1  0  0\n",
      " 0  0  0  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  1  0  0\n",
      " 0  0  0  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  0  1  0\n",
      " 0  0  0  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  0  1  0\n",
      " 0  0  0  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  0  0  1\n",
      " 0  0  0  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  0  0  1\n",
      " 0  0  0  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  0  0  0\n",
      " 1  0  0  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  0  0  0\n",
      " 1  0  0  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  0  0  0\n",
      " 0  1  0  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  0  0  0\n",
      " 0  1  0  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  0  0  0\n",
      " 0  0  1  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  0  0  0\n",
      " 0  0  1  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  0  0  0\n",
      " 0  0  0  1\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  0  0  0\n",
      " 0  0  0  1\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      "[torch.DoubleTensor of size 2x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient check passes!\n",
    "input = (Variable(torch.randn(2,4).double(), requires_grad=True),)\n",
    "assert(gradcheck(MyReLU(), input, eps=1e-6, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_model(n_features, n_classes, nonlinearity=MyReLU())\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "_, H_relu = train(step_fnc=sgd_step, loss=loss, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement “ReQU” unit\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "Here, we’ll implement a made-up activation function that we’ll call the Rectified Quadratic Unit\n",
    "(ReQU). Like the sigmoid and ReLU and several others, it is applied element-wise to all its\n",
    "inputs:\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "$$z_i = I[x_i > 0] x_i ^ 2$$\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "(Note, exercise is taken from  https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/practicals/practical4.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hint: use ReLU implementation as a template\n",
    "\n",
    "class ReQU(torch.autograd.Function):\n",
    "  def forward(self, input):\n",
    "    self.save_for_backward(input)\n",
    "    ??\n",
    "\n",
    "  def backward(self, grad_output):\n",
    "    \"\"\"\n",
    "    In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "    with respect to the output, and we need to compute the gradient of the loss\n",
    "    with respect to the input.\n",
    "    \"\"\"\n",
    "    input, = self.saved_tensors\n",
    "    ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## GradCheck\n",
    "input = (Variable(torch.randn(20,20).double(), requires_grad=True),)\n",
    "assert(gradcheck(ReQU(), input, eps=1e-6, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_model(n_features, n_classes, nonlinearity=ReQU())\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "_, H_requ = train(step_fnc=sgd_step, loss=loss, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Comparing ReLU and ReQU\n",
    "plt.plot(H_relu, label=\"ReLU\")\n",
    "plt.plot(H_requ, label=\"ReQU\")\n",
    "plt.ylim([0, 10])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = {}\n",
    "\n",
    "input = (Variable(torch.randn(20,20).double(), requires_grad=True),)\n",
    "result['requ'] = 1.0*int(gradcheck(ReQU(), input, eps=1e-6, atol=1e-4))\n",
    "\n",
    "# If code is slow, you can use build_logreg !\n",
    "model = build_logreg(784, 10) \n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "init_w = get_model_weights(model)\n",
    "down = 0\n",
    "for _ in range(10):\n",
    "    loss_before = L(w=init_w, model=model, loss=loss)\n",
    "    set_model_weights(model=model, w=init_w)\n",
    "    v = ES_grad(model, loss, x_tr=x_train, y_tr=y_train, sigma=0.002, N=50)\n",
    "    loss_after = L(w=init_w - 0.1*v, model=model, loss=loss)\n",
    "    if loss_before > loss_after:\n",
    "        down += 1\n",
    "# This code is stochastic, so of course sometimes this assert can fail. \n",
    "result['ES'] = int(down > 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(result, open(\"4a_computing_gradient.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
