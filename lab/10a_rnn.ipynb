{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to RNNs\n",
    "\n",
    "Goal of the lab is to:\n",
    "    * Implement a simple RNN\n",
    "    * Understand vanishing gradients in RNNs\n",
    "    * Revisit code conventions: PyTorch data loader, functional model construction, more standard trainig loop\n",
    "    \n",
    "References:\n",
    "    * Content heavily based on https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents-notebooks/402_RNN.ipynb (I highly recommend the whole repository)\n",
    "    * http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whiteboard exercises\n",
    "\n",
    "(Plus any left out exercise from the previous labs)\n",
    "\n",
    "<img width=500 src=\"http://www.wildml.com/wp-content/uploads/2015/10/rnn-bptt-with-gradients.png\">\n",
    "\n",
    "* (0.5) Find  in literature at least two ways to combat vanishing gradients in RNNs *without* changing the architecture. Describe them and argue why they should work. \n",
    "\n",
    "* (0.5) Describe how Truncated Back Propagation Through Time (TBPTT) would work for the network in the figure. Argue why TBPTT, with a correctly chosen cutoff, can be an effective method (e.g. not leading to inaccurate gradients and not slowing convergence) of training?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters (constant for the notebook)\n",
    "EPOCH = 1               # train the training data n times, to save time, we just train 1 epoch\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "TIME_STEP = 28          # rnn time step / image height\n",
    "INPUT_SIZE = 28         # rnn input size / image width\n",
    "LR = 0.01               # learning rate\n",
    "DOWNLOAD_MNIST = True   # set to True if haven't download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A standard way to load a dataset\n",
    "train_data = dsets.MNIST(\n",
    "    root='./mnist/',\n",
    "    train=True,                         # this is training data\n",
    "    transform=transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n",
    "                                        # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n",
    "    download=DOWNLOAD_MNIST,            # download it if you don't have it\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# convert test data into Variable, pick 2000 samples to speed up testing\n",
    "test_data = dsets.MNIST(root='./mnist/', train=False, transform=transforms.ToTensor())\n",
    "# shape (2000, 28, 28) value in range(0,1)\n",
    "test_x = Variable(test_data.test_data, volatile=True).type(torch.FloatTensor)[:2000]/255.   \n",
    "test_y = test_data.test_labels.numpy().squeeze()[:2000]    # covert to numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Code your own RNN\n",
    "\n",
    "Code your own RNN. It will be defined by two layers:\n",
    "\n",
    "* i2h: takes *input and hidden state from the previous step* and *produce new hidden state* using equation $$h_{t+1}=tanh(W input_{t+1} + U h_{t})$$\n",
    "\n",
    "* h2o: taken hidden state and produces $$o_{t+1} = softmax(Vh_{t+1})$$ (note that in pytorch softmax is applied within the cross entropy loss function)\n",
    "\n",
    "Tasks:\n",
    "\n",
    "* Fill up missing blanks to train your own RNN!\n",
    "* Save results as a figure (10a_1.png)\n",
    "\n",
    "Starting code is provided.\n",
    "\n",
    "Expected outcome:\n",
    "\n",
    "<img width=300 src=\"https://raw.githubusercontent.com/gmum/nn2018/master/lab/fig/10/ex1_expected.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, init_strategy=\"simple\", init_scale=0.01):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = ?? # Hint: linear module, from (input size + hidden_size) to hidden_size\n",
    "        self.h2o = ?? # Hint: linear module\n",
    "        \n",
    "        if init_strategy == \"simple\":\n",
    "            self.i2h.weight.data.normal_(0, init_scale)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        self.i2h.bias.data.fill_(0)\n",
    "        self.h2o.weight.data.uniform_(0, init_scale)\n",
    "        self.h2o.bias.data.fill_(0)\n",
    "            \n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = ?? # Hint: use torch.cat to combine input and hidden input 2d vector\n",
    "        hidden = F.tanh(??) # Hint: use input to hidden\n",
    "        output = ?? # Hint: use hidden to output\n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return Variable(torch.zeros(batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = RNN(28, 64, 10)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training and testing\n",
    "H = {\"acc\": []}\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (x, y) in enumerate(train_loader):        # gives batch data\n",
    "        b_x = Variable(x.view(-1, 28, 28))              # reshape x to (batch, time_step, input_size)\n",
    "        b_y = Variable(y)                               # batch y\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden = rnn.initHidden(b_x.size()[0])\n",
    "        for i in range(??): # Hint: iterate through all the steps\n",
    "            ?? # Hint: just apply forward from the model\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            hidden = rnn.initHidden(test_x.size()[0])\n",
    "            for i in range(test_x.size()[1]):\n",
    "                test_output, hidden = rnn(test_x[:, i], hidden)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "            accuracy = sum(pred_y == test_y.reshape(-1,)) / float(test_y.size)\n",
    "            H['acc'].append(accuracy)\n",
    "            print('Epoch: ', epoch + step*len(b_x)/2000., '| train loss: %.4f' % loss.data[0], '| test accuracy: %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"Simple(0.01)\")\n",
    "plt.plot(H['acc'])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training accuracy\")\n",
    "plt.savefig(\"10a_1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Vanishing Gradient\n",
    "\n",
    "\n",
    "* Measure ||dL/dh_i|| for different states, where ||.|| is the euclidean norm of the vector. \n",
    "* Plot run for the simple initialization (with scale=0.001). Save figure to (10a_2_simple.png)\n",
    "\n",
    "Starting code is provided\n",
    "\n",
    "Expected (it can be different for you due to randomness) outcome (x axis is epoch, y axis is ||dL/dh_i||, color is index):\n",
    "\n",
    "<img width=300 src=\"https://raw.githubusercontent.com/gmum/nn2018/master/lab/fig/10/ex2_expected_simple.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H = defaultdict(list)\n",
    "\n",
    "rnn = RNN(28, 64, 10, \"simple\", 0.001)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\n",
    "    \n",
    "for epoch in range(EPOCH):\n",
    "    for step, (x, y) in enumerate(train_loader):        # gives batch data\n",
    "        b_x = Variable(x.view(-1, 28, 28))              # reshape x to (batch, time_step, input_size)\n",
    "        b_y = Variable(y)                               # batch y\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make sure gradients are retained\n",
    "        hidden = rnn.initHidden(b_x.size()[0])\n",
    "        hiddens = [hidden]\n",
    "        for i in range(??): # (from the previous exercise)\n",
    "            ?? # (from the previous exercise)\n",
    "            hiddens.append(hidden)\n",
    "            ?? # google \"retain_grad\". Otherwise hidden.grad=None. \n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward(retain_variables=True)      # backpropagation, compute gradients\n",
    "        \n",
    "        # Save ||dL/dh_i||\n",
    "        dLdhi = []\n",
    "        for h in hiddens[1:]:\n",
    "            dLdhi.append(??) # Hint: Just compute ||dL/dh_i|| from h.grad.data.numpy() using np.linalg.norm + average over examples\n",
    "        \n",
    "        optimizer.step()                                \n",
    "\n",
    "        if step % 50 == 0:\n",
    "            hidden = rnn.initHidden(test_x.size()[0])\n",
    "            for i in range(test_x.size()[1]):\n",
    "                test_output, hidden = rnn(test_x[:, i], hidden)\n",
    "            \n",
    "            for i, val in enumerate(dLdhi):\n",
    "                H['dL/dh_{}'.format(i)].append(val)\n",
    "                \n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "            accuracy = sum(pred_y == test_y.reshape(-1,)) / float(test_y.size)\n",
    "            H['acc'].append(accuracy)\n",
    "            print('Epoch: ', epoch + step*len(b_x)/2000., '| train loss: %.4f' % loss.data[0], '| test accuracy: %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "cm = plt.get_cmap(\"coolwarm\", 80)\n",
    "plt.title(\"Simple(0.01)\") \n",
    "for i in range(28):\n",
    "    plt.plot(H['dL/dh_{}'.format(i)], color=cm(i/28.))\n",
    "print(\"Ratio of ||dL/dh_27|| to ||dL/dh_1|| is \" + str(np.mean(H['dL/dh_27'][-10:-1]) / np.mean(H['dL/dh_1'][-10:-1])))\n",
    "plt.savefig(\"10a_2_simple.png\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
