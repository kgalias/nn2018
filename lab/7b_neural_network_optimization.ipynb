{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Neural Networks\n",
    "\n",
    "To remind, necessary ingredients to train NN:\n",
    "    * model\n",
    "    * objective\n",
    "    * optimizer\n",
    "    \n",
    "Today we will try to understand basics of optimization of neural networks, giving context for the last two lectures. Goal is to:\n",
    "* Understand basics of generalization, and the difference between optimization and generalization (more on that in \"Understanding generalization\" lab)\n",
    "* Understand impact of hyperparameters in SGD on:\n",
    "\n",
    "  - generalization (lr, batch size)\n",
    "  - speed of optimization (lr, momentum, batch size) \n",
    "\n",
    "References:\n",
    "* Deep Learning book chapter on optimization: http://www.deeplearningbook.org/contents/optimization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Boilerplate code to get started\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "from src import fmnist_utils\n",
    "from src.fmnist_utils import *\n",
    "\n",
    "def plot(H):\n",
    "    plt.title(max(H['test_acc']))\n",
    "    plt.plot(H['acc'], label=\"acc\")\n",
    "    plt.plot(H['test_acc'], label=\"test_acc\")\n",
    "    plt.legend()\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fmnist_utils.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: optimization speed\n",
    "\n",
    "Assuming fixed number of *epochs*, it is usually better to use either smaller batch size, or larger learning rate. Theoretical reason for it is not completely clear, so let's focus in this exercise on an empirical investigation.\n",
    "\n",
    "Assume you are allowed to train the given network for 10 epochs. Answer the following questions:\n",
    "\n",
    "* a) What was the optimal $\\eta$ (assuming $S$=128 and $\\mu$=0.9) for the final training accuracy?\n",
    "* b) Did it also provide the best test accuracy? If yes, why (hint: consider if model is under or over-fitting)?\n",
    "* c) What is the optimal $S$ (assuming $\\eta$=0.1 and $\\mu$=0.9) for the final training accuracy?\n",
    "* d) Why is higher learning rate, or smaller batch size, optimizing faster? Give your best explanation (it can be hypothetical, there is no obvious theoretical answer)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Some starting code. Loop through different LR and BS and find optimal ones.\n",
    "model = build_mlp(784, 10, hidden_dim=512)\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.2, momentum=0.9)\n",
    "H = train(loss=loss, model=model, x_train=x_train, y_train=y_train,\n",
    "          x_test=x_test, y_test=y_test,\n",
    "          optim=optimizer, batch_size=128, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = {\"a\": \"\", \"b\": \"\", \"c\": \"\", \"d\": \"\"}\n",
    "json.dump(answers, open(\"7b_ex1.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: generalization\n",
    "\n",
    "Story with generalization is also unclear, but it is generally accepted that higher noise levels in SGD lead to better generalization. Think of noise in optimization (leading to low fidelity, as seen in lab 7a, for instance) as a close analog of typical regularizations (like dropout or batch normalization, that we will discuss next time).\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "a) Check a range of LR and BS and find the best generalizing combination of LR and BS. What test accuracy were you able to achieve? What is the best LR and BS combination?\n",
    "\n",
    "b) Answer the following question: Is stability correlated with using large LR or small BS. If yes, what is the intuitive reason for it? Feel free to give a hypothesis.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* Make sure you achieve 100% training accuracy with each run, discard hyperparameters that are not achieving convergence.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Do not change the model in the starting code. It is on purpose a bit more complex MLP.\n",
    "\n",
    "* You can measure stability by computing margin. This is implemented for you (using DeepFool method, https://arxiv.org/abs/1511.04599). Measuring margin is expensive, so recommended approach would be to compute it only on few final runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = {\"a\": \"\", \"b\": \"\", \"c\": \"\"}\n",
    "json.dump(answers, open(\"7b_ex2.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability measure\n",
    "\n",
    "In 7a lab we discussed bias/variance view. Here, we will take a stability based view. To estimate stability, \n",
    "we will record maximum change in prediction when adding gaussian noise to examples. This is a very rudimentary\n",
    "way to estimate geometric margin of the network, and we will talk more about this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named stability",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-f4a7f975f250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmeasure_stability_deepfool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named stability"
     ]
    }
   ],
   "source": [
    "from src.stability import measure_stability_deepfool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal $\\eta$ and $S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:22<00:00, 17.97it/s]\n"
     ]
    }
   ],
   "source": [
    "## Starting code\n",
    "\n",
    "Hs = []\n",
    "Lrs = [0.1]\n",
    "Margins = []\n",
    "\n",
    "for lr in Lrs:\n",
    "    model = build_mlp(784, 10, hidden_dims=[100, 100, 100])\n",
    "    loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    H = train(loss=loss, model=model, x_train=x_train, y_train=y_train,\n",
    "              x_test=x_test, y_test=y_test,\n",
    "              optim=optimizer, batch_size=100, n_epochs=400)\n",
    "    Margins.append(measure_stability_deepfool(model=model, \n",
    "                x_train=x_train, y_train=y_train, loss=loss, N=1000))\n",
    "    Hs.append(H)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
